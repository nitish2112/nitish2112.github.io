<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nitish Srivastava on Nitish Srivastava</title>
    <link>https://nitish2112.github.io/</link>
    <description>Recent content in Nitish Srivastava on Nitish Srivastava</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Nitish Srivastava</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>MatRaptor: A Sparse-Sparse Matrix Multiplication Accelerator Based on Row-Wise Product</title>
      <link>https://nitish2112.github.io/publication/matraptor/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://nitish2112.github.io/publication/matraptor/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tensaurus: A Versatile Accelerator for Mixed Sparse-Dense Tensor Computations.</title>
      <link>https://nitish2112.github.io/publication/tensaurus/</link>
      <pubDate>Sun, 22 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://nitish2112.github.io/publication/tensaurus/</guid>
      <description></description>
    </item>
    
    <item>
      <title>T2S-Tensor: Productively Generating High-Performance Spatial Hardware for Dense Tensor Computations</title>
      <link>https://nitish2112.github.io/publication/t2s-tensor/</link>
      <pubDate>Wed, 17 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://nitish2112.github.io/publication/t2s-tensor/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Operation Dependent Frequency Scaling Using Desynchronization</title>
      <link>https://nitish2112.github.io/publication/desynchronization/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://nitish2112.github.io/publication/desynchronization/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rosetta: A Realistic High-Level Synthesis Benchmark Suite for Software Programmable FPGAs</title>
      <link>https://nitish2112.github.io/publication/rosetta/</link>
      <pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://nitish2112.github.io/publication/rosetta/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Tutorial on the Gem5 Minor CPU Model</title>
      <link>https://nitish2112.github.io/post/gem5-minor-cpu/</link>
      <pubDate>Mon, 10 Jul 2017 10:09:32 -0400</pubDate>
      
      <guid>https://nitish2112.github.io/post/gem5-minor-cpu/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;This is an introduction tutorial on gem5 minor cpu model.&lt;/p&gt;

&lt;p&gt;Many a times it gets difficult for the computer architects to get started with event-driven simulators. This document is written to target that audience and provide an overview of the minor cpu model in gem5 which implements an in-order pipelined processor. If you have never worked on event-driven simulators and don&amp;rsquo;t know what they are, there is a video &lt;a href=&#34;https://www.youtube.com/watch?v=irbshkdVFao&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. This tutorial will help the reader to understand how the event-driven minor cpu model is implemented in gem5 and will not go much into details of how to compile and build gem5, how to add tracing and what are ports and how do they work. This information can be found in &lt;a href=&#34;http://learning.gem5.org/book/index.html&#34; target=&#34;_blank&#34;&gt;Learning Gem5&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;minor-cpu&#34;&gt;Minor CPU&lt;/h1&gt;

&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;

&lt;p&gt;It is a 4 stage pipelined processor. The four stages being fetch1, fetch2, decode and execute. As opposed to 5-stage DLX pipeline that every computer architecture student is familiar with, this is somewhat different. The ITLB access, and fetch of the instruction from the main memory are done in fetch1. fetch2 is responsible for decoding the instruction, decode is responsible for just some book-keeping ( why this is a stage I am not sure at this point ) and execute implements the logic for issue, execute, memory, writeback and commit. All of these stages are defined as SimObjects in the class Pipeline which implements the entire pipeline. The different pipeline stages are connected via Latches ( we will talk about their implementation later in this tutorial ).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;class Pipeline {
    /* Latches to connect the stages */
    Latch&amp;lt;ForwardLineData&amp;gt; f1ToF2;  
    Latch&amp;lt;BranchData&amp;gt; f2ToF1;
    Latch&amp;lt;ForwardInstData&amp;gt; f2ToD;
    Latch&amp;lt;ForwardInstData&amp;gt; dToE;
    Latch&amp;lt;BranchData&amp;gt; eToF1;

    /* Pipeline Stages */
    Execute execute; 
    Decode decode;     
    Fetch2 fetch2;
    Fetch1 fetch1

    /* Action to be performed at each cycle (tick) */
    void evaluate();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;execute&#34;&gt;Execute&lt;/h2&gt;

&lt;p&gt;The way to think about an object/class is in terms of its data members, as they correspond to the physical data-structures that you will build in your hardware. The methods tell how these objects interact, which somewhat represents the wiring and the control. The main objects in the Execute stage are shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;class Execute {
    Latch&amp;lt;ForwardInstData&amp;gt;::Output inp; // connected to dToE Latch
    Latch&amp;lt;BranchData&amp;gt;::Input out; // connected to eToF1 Latch
    /** Scoreboard of instruction dependencies */
    std::vector&amp;lt;Scoreboard&amp;gt; scoreboard;
    /** The execution functional units */
    std::vector&amp;lt;FUPipeline *&amp;gt; funcUnits;
    std::vector&amp;lt;InputBuffer&amp;lt;ForwardInstData&amp;gt;&amp;gt; inputBuffer;
    void evaluate();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Apart from the data-members, there is an evaluate() method which is the actiopn this stage has to perform at each CPU tick. As it can be seen from the implementation of Execute class that there are two wires one for input and one for output. The one named &amp;ldquo;inp&amp;rdquo; is connected to the output of dToE Latch and the one named &amp;ldquo;out&amp;rdquo; is connected to eToF1 Latch by the constructor of Pipeline class. The dToE Latch carries the instructions from decode to execute and eToF1 carries branch updates (the branch outcome is known only in execute) to the fetch1 stage. There is vector of objects of the class Scoreboard. Each element in the vector corresponds to a scoreboard for a thread. As the processor can be multithreaded this is necessary to seperate the scoreboards of different threads. In the rest of this tutorial, I will assume a single threaded processor, so only scoreboard[0] will be a valid entry. Same is the case for the funcUnits which is a vector of functional unit pipelines for different threads and inputBuffer. The figure below gives a pictorial representation of how the execute stage looks like.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;execute.png&#34; alt=&#34;execute&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;decode&#34;&gt;Decode&lt;/h2&gt;

&lt;p&gt;The important data-structures and methods in the decode class are shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;class Decode {
    Latch&amp;lt;ForwardInstData&amp;gt;::Output inp; // connected to f2ToD
    Latch&amp;lt;ForwardInstData&amp;gt;::Input out;  // connected to dToE
    /* references to execute.inputBuffer vector */
    std::vector&amp;lt;InputBuffer&amp;lt;ForwardInstData&amp;gt;&amp;gt; &amp;amp;nextStageReserve;  
    std::vector&amp;lt;InputBuffer&amp;lt;ForwardInstData&amp;gt;&amp;gt; inputBuffer;
    std::vector&amp;lt;DecodeThreadInfo&amp;gt; decodeInfo;
    void evaluate();

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Similar to execute stage, decode contains one input wire and one output wire, but this time the input wire is carrying the decoded instruction from fetch2 to decode and the output wire is carrying the output instruction from decode to execute. As decode does not determine the outcome of branches there is no wire going from decode to fetch1 as in the case of execute. decode also contains an alias ( reference ) to the inputBuffer object of the execute stage. This reference is used to reserve an entry in the inputBuffer of the execute stage. By doing this decode makes sure that whatever instruction it is inserting in the dToE latch will have a place in the input buffer of the execute from which it executes the instructions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;decode.png&#34; alt=&#34;decode&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;fetch2&#34;&gt;Fetch2&lt;/h2&gt;

&lt;p&gt;The important data-structures and methods in fetch2 class are shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;class Fetch2 {
  Latch&amp;lt;ForwardLineData&amp;gt;::Output inp;  // connected to f1ToF2
  Latch&amp;lt;BranchData&amp;gt;::Output branchInp; // connected to eToF1
  Latch&amp;lt;BranchData&amp;gt;::Input predictionOut; // coneected to f2ToF1
  Latch&amp;lt;ForwardInstData&amp;gt;::Input out;      // connected to f2ToD
  
  std::vector&amp;lt;InputBuffer&amp;lt;ForwardInstData&amp;gt;&amp;gt; &amp;amp;nextStageReserve
  std::vector&amp;lt;InputBuffer&amp;lt;ForwardLineData&amp;gt;&amp;gt; inputBuffer;
  std::vector&amp;lt;Fetch2ThreadInfo&amp;gt; fetchInfo;

  BPredUnit &amp;amp;branchPredictor;   // Branch Predictor

  void evaluate();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By now it should be clear what are the different components in the pipeline stages are. First of all there are some wires which are either input to the stage (and hence the Output of the Latch) or are outputs from the stage (Inputs to the Latches). Then there is an instruction buffer for each stage which holds entries of ForwardInstData or ForwardLine data which are just instructions wrapped in a class. Then at last there is a reference to the input buffer of the next stage and an evaluate() function which gets executed every clock tick. The main function that fetch2 is responsible for is decoding the instruction. The names can be sometimes misleading as the decode stage is not the one that does the real decoding. The decoding is indeed done in the fetch2 stage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;fetch2.png&#34; alt=&#34;fetch2&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;fetch1&#34;&gt;Fetch1&lt;/h2&gt;

&lt;p&gt;Not being so verbose fetch1 looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;class Fetch1 {
  Latch&amp;lt;BranchData&amp;gt;::Output inp;        // connected to eToF1
  Latch&amp;lt;ForwardLineData&amp;gt;::Input out;    // connected to f1ToF2
  Latch&amp;lt;BranchData&amp;gt;::Output prediction; // connected to f2ToF1
  
  std::vector&amp;lt;InputBuffer&amp;lt;ForwardLineData&amp;gt;&amp;gt; &amp;amp;nextStageReserve;
  
  IcachePort icachePort;

  FetchQueue requests;
  FetchQueue transfers;
  IcacheState icacheState;
  InstSeqNum lineSeqNum;

  void evaluate();
  std::vector&amp;lt;Fetch1ThreadInfo&amp;gt; fetchInfo;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fetch1 is responsible for doing ITLB and ICache access. icachePort provides the interface between cache and fetch1. To learn more about ports refer &lt;a href=&#34;http://learning.gem5.org/book/part2/memoryobject.html#define-a-slave-port-type&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Overall the pipeline looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;pipeline.png&#34; alt=&#34;pipeline&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;the-execution-of-the-pipeline&#34;&gt;The execution of the pipeline&lt;/h1&gt;

&lt;p&gt;Now that we are aware of what are the key data-structures in each stage and how the CPU is connected to the pipeline and threads, we can look into how does the pipeline operate. Each pipeline has an event associated with it called &amp;ldquo;event&amp;rdquo; which is scheduled for every clock tick. Whenever the event fires, the evaluate() function of the pipeline class is called. The evaluate method of the pipeline class calls the evaluate method on each of the pipeline stages in the reverse order. The order here is important because the updates from the later stages of the pipeline should be visible to the earlier stages of the pipeline. Think of a very simple stall logic, if the execute stage decides to stall in the current cycle, fetch1, fetch2, and decode all should stall and should not change any state in the current cycle. This would not be possible if we evaluate the fetch1, fetch2 and decode before evaluating execute stage. The converse of this is however not true, as in conventional processor pipelines there are no feed-forward paths and hence the evaluation of later stages do not depend on the evaluation of earlier stages in the pipeline. Once all the stages are updated in the reverse order, the latches are advanced, which means in the two element buffer, the tail is pushed to the head and a space for new entry is made at the tail. The code looks somewhat like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;void                                                                     
Pipeline::evaluate()                                                     
{                                                                        
    /* Note that it&#39;s important to evaluate the stages in order to allow 
     *  &#39;immediate&#39;, 0-time-offset TimeBuffer activity to be visible from
     *  later stages to earlier ones in the same cycle */                
    execute.evaluate();                                                  
    decode.evaluate();                                                   
    fetch2.evaluate();                                                   
    fetch1.evaluate();                                                                                                     
                                                                         
    /* Update the time buffers/latches after the stages */               
    f1ToF2.evaluate(); 
    f2ToF1.evaluate();                                                   
    f2ToD.evaluate();                                                    
    dToE.evaluate();                                                     
    eToF1.evaluate();  
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the Latches &amp;ldquo;must&amp;rdquo; be updated after all the stages as otherwise the stages will start popping wrong data from the Latches. The order of updates within the Latches do not matter.&lt;/p&gt;

&lt;p&gt;Now that we have settled on what the order of updates for the pipeline stages should be lets delve deeper into what the evaluate() method for each of the pipeline stage is doing.&lt;/p&gt;

&lt;h2 id=&#34;execute-evaluation&#34;&gt;Execute evaluation&lt;/h2&gt;

&lt;p&gt;The evaluate method for the execute stage looks something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;void 
Execute::evaluate()
{
    // Set the dToE Latch data as the one to be pushed into input buffer next
    inputBuffer[inp.outputWire-&amp;gt;threadId].setTail(*inp.outputWire);

    lsq.step(); // Step the Load-Store Queues
    commit();   // Commit the instruction 
    issue();    // Issue instructions whose dependencies are satisfied 

    // Push the dToE Latch data into the input buffer
    inputBuffer[inp.outputWire-&amp;gt;threadId].pushTail();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The commit method looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;/** Commit takes an inst out from the head of the inflight inst queue and
  * depending upon whether out of order memory requests are supported or
  * not it commits the instruction, clears its destination entry from 
  * scorebiard and remove it from the head of the inflight insts 
*/
void
Execute::commit(){
    // While issue() has already issued some instructions in the FUs 
    while (executeInfo[tid].inFlightInsts-&amp;gt;empty() 
             &amp;amp;&amp;amp; num_insts_committed != commitLimit){

        head_inflight_inst = executeInfo[tid].inFlightInsts.front();
        inst = head_inflight_inst-&amp;gt;inst;
        mem_response = ( head_inflight_inst-&amp;gt;inst-&amp;gt;inLSQ ) ? 
                                     lsq.findResponse() : NULL;

        if (mem_response)
            handleMemResponse(inst, mem_response, ...);
        else {
	    // If there is a load/store inflight try to commit it before the
	    // the head of the inflight inst
            if (!executeInfo[tid].inFUMemInsts-&amp;gt;empty() &amp;amp;&amp;amp; lsq.canRequest()) {
                fun_inst = execeuteInfo.inFUMemInsts.front().inst;
                fu = funcUnits[ fun_inst-&amp;gt;fuIndex ];
                if (!fu_inst-&amp;gt;inLSQ &amp;amp;&amp;amp; fu_inst-&amp;gt;canEarlyIssue )
                {
                    try_to_commit = true;
                    inst = fu_inst;
                }
            }
	    // At this point depending on whether we are doing an early
	    // issue of mem request or actually handling the head of the 
	    // inflight inst. inst will be pointing to the corresponding inst
            if (!completed_inst &amp;amp;&amp;amp; !inst-&amp;gt;inLSQ) 
            {
                fu_inst = funcUnits[inst-&amp;gt;fuIndex]-&amp;gt;front();
                if (fu_inst.inst-&amp;gt;id == inst-&amp;gt;id){
                    try_to_commit = true;
                    completed_inst = true;
                }
            }
            if (try_to_commit)
            {
		// If there is a stream sequence mismatch i.e. inst is a post
		// branch inst, then discard the inst.
                discard_inst = 
                     inst-&amp;gt;id.streamSeqNum != executeInfo[tid].streamSeqNum;
                if (!discard_inst)
                    completed_inst = commitInst(...);
            
            }
            if (completed_inst)
            {
                funcUnits[inst-&amp;gt;fuIndex]-&amp;gt;stalled = false;
                executeInfo[tid].inFlightInsts-&amp;gt;pop();
                scoreboard[tid].clearInstDests(inst);
            }             
        }
    }                 
}

LSQ::findResponse(inst) {
  if (!transfers.empty()) {
    LSQRequest* req = transfers.front();
    if ((req-&amp;gt;inst-&amp;gt;id == inst-&amp;gt;id) &amp;amp;&amp;amp; // same instruction
        (req-&amp;gt;isComplete()) ) { // request must be complete
       return req;
    }
  }
  return NULL;
}

Execute::handleMemResponse (inst, LSQRequest mem_response, ...) {
  inst-&amp;gt;staticInst-&amp;gt;completeAcc(...);
  lsq.popResponse();
  ...
}

LSQ::popResponse(...){
  transfers.pop();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;void 
Execute::commitInst( inst, branch )
{
    if (inst-&amp;gt;isMemRef()) // load/store instruction
    {
        inst-&amp;gt;staticInst-&amp;gt;initiateAcc(); // this will eventually push inst on LSQ&#39;s request queue
	completed_inst = true;
    }
    else  // other arithmetic instructions
    {
	// Execute the instruction and write the result in register file
	// If the instruction is a branch instruction it updates the thread._pcState
	inst-&amp;gt;staticInst-&amp;gt;execute();

	target = thread-&amp;gt;pcState();
	pc_before = inst-&amp;gt;pc;
	if ( inst-&amp;gt;predictedTaken ){
	   if ( inst-&amp;gt;predictedTarget == target )
		// .. update branch variable for corect prediction
	   else
		// .. update for wrongly predicted 
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;/** Takes a set of instructions out to the inputBuffer. If the dependencies
 *  are satisfied keeps issuing the instructions bhy pushing them to FU 
 *  and marking the dests in scoreboard. It stops and removes the instruction
 *  from the inputBuffer in case all the insts in Latch are sent to the FUs. It
 *  also stops in the case when any of the inst is dependent on some previous
 *  inst. In this case any of insts after that are also not scheduled
*/
void 
Execute::issue ( inst )
{
    inst_in = inputBuffer[tid].front();

    do {
	issued = false;
	inst = insts_in-&amp;gt;insts[thread.inputIndex];
	for ( fu_idx = 0; fu_idx &amp;lt; numFuncUnits; fu_idx++ ){
	    fu = funcUnits[fu_idx];
	    if ( !fu-&amp;gt;stalled &amp;amp;&amp;amp; fu-&amp;gt;provides(inst) ){
		// Check scoreboard to see if inst depends on previous insts
		if ( scoreBoard.canInstIssue(inst) ){
		    fu-&amp;gt;push( inst );
		    // Mark the destination regs in the scoreboard
		    scoreBoard.markupInstDest(inst); 
		    executeInfo[tid].inFlightInsts.push(inst);
		    issued = true;
		}
	    }
	}
	if (executeInfo[tid].inputIndex == insts_in-&amp;gt;width() ){
	    inputBuffer[tid].pop();	
	    inst_in = NULL;
	}
    } while ( inst_in &amp;amp;&amp;amp; issued )
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fetch2-evaluation&#34;&gt;Fetch2 evaluation&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;void
fetch2::evaluate()
{
    // Mark the data in the f1ToF2 Lacth as the one to be 
    // pushed to the inputBuffer next
    inputBuffer[tid].setTail(*inp.OutputWire); // f1ToF2

    ForwardInstData &amp;amp;insts_out = *out.inputWire; // f2ToD
    BranchData &amp;amp;branch_inp = *branchInp.outputWire; // eToF1

    // React to branches from execute stage to update local branch
    // prediction structures (update the branch predictor itself)
    updateBranchPrediction(branch_inp);    

    // thread will be blocked if no space in decode&#39;s inputBuffer
    fetchInfo[tid].blocked = !nextStageReserve[tid].canReserve();

    if ( fetchInfo[tid].expectedStreamSeqNum == inputBuffer[tid].front()-&amp;gt;id.streamSeqNum 
	&amp;amp;&amp;amp; fetchInfp[tid].predictionSeqNum != inputBuffer[tid].front()-&amp;gt;id.predictionSeqNum )
		inputBuffer[tid].pop();

    line_in = inputBuffer[tid].front();
    
    // Discard the instructions for which Fetch2 predicted sequence number is 
    // different from the one with fetch1 fetched these instructions i.e. discard 
    // the instructions which are fetched not complying to branch pred decision in fetch2
    if ( line_in &amp;amp;&amp;amp; fetchInfo[tid].expectedStreamSeqNum == line_in-&amp;gt;id.streamSeqNum 
              &amp;amp;&amp;amp; fetchInfo[tid].predictionSeqNum != line_in-&amp;gt;id.predictionSeqNum) {
        inputBuffer.pop();
    }

    // fetch1 sends an entire caache line to fetch2 and not just a single inst
    // fetch2 depending on what the output width is decodes that many insts.
    // This decoding in hardware can be done using multiple decoders or
    // a single decoder time-multiplexed. 
    while ( line_in &amp;amp;&amp;amp; fetchInfo[tid].inputIndex &amp;lt; line_in-&amp;gt;lineWidth
             &amp;amp;&amp;amp; outputIndex &amp;lt; outputWidth )
    {
	fetchInfo.pc = line_in-&amp;gt;pc;
        dyn_inst = new MinorDynInst(line_in-&amp;gt;id);
	// Decode the instruction
        decoded_inst = decoder-&amp;gt;decode(fetchInfo[tid].pc);
	// Advance the PC
        TheISA::advancePC (fetchInfo[tid].pc, decoded_inst);
	// Calls branchPredictor.predict() if the inst is a Control inst
	// and updates prediction variable for new stream and prediction sequence numbers
        predictBranch(dyn_inst, prediction);
	insts_out.insts[output_index++] = dyn_inst;
	if ( !prediction.isBubble() )
	    line_in = NULL;
	else if ( fetchInfp[tid[.inputIndex == line_in-&amp;gt;lineWidth ) 
	    inputBuffer[tid].pop();
   }
   if ( !inst_out.isBubble() )
        nextStageReserve[tid].reserve();

   if ( !inp.outputWire-&amp;gt;isBubble )
        inputBuffer[inp.outputWire-&amp;gt;id.threadId].pushTail();        
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fetch1-evaluation&#34;&gt;Fetch1 evaluation&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;void
fetch1::evaluate()
{
    BranchData &amp;amp;execute_branch = *inp.outputWire;  // eToF1
    BranchData &amp;amp;fetch2_branch = *prediction.outputWire; // f2ToF1
    ForwardLineData &amp;amp;line_out = *out.inputWire; // f1ToF2

    fetchInfo[tid].blocked = !nextStageReserve[tid].canReserve();

    // Prioritize the branch stream change of execute over fetch2
    if ( execute_branch.isStreamChange() )
    {
	// updates the fetchInfo[tid].pc to branch target. updates the stream 
	// sequence number and prediction sequence number in fetcInfo[tid]
	changeStream( execute_branch )
    }
    else if ( fetch2_branch.isStreamChange() )
    {
	changeStream( fetch2_branch );
    }

    // Pushes the fetch memory request to the request queue, reserves a slot for
    // it in the transfers queue and does an ITLB access. 
    // It also updates the PC by +4 ( as no branch predictor in fetch1 ) 
    fetchLine(tid);

    nextStageReserve.reserve();

    // If the ICache is not busy, it tries to send fetch request to ICache
    // if successful it moves the request from request queue to transfers queue.
    stepQueues();

    // The head of the transfers queue is a completed fetch request
    if ( !transfers.empty() &amp;amp;&amp;amp; transfer.front()-&amp;gt;isComplete() )
    {
	line_out.pc = transfers.front().pc;
	line_out.line = transfers.front()-&amp;gt;data;
	popAndDiscard(transfers);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;instructions&#34;&gt;Instructions&lt;/h1&gt;

&lt;p&gt;Each cpu has its own class for dynamic instruction for e.g. for minor cpu this is defined in dyn_inst.cc as class MinorDynInst. Each Dynamic instruction class has an object of class StaticInst. StaticInst is an abstract class whose execute() method is purely virtual. It provides a base abstract class for all the different kinds of instructions in the ISA. Based on which ISA you are compiling gem5 with, the build system generates a class for each of the instructions in the ISA which is derived from StaticInst class and it has the execute method which can be used to execute the instruction and update the register file. Go to the file build/RISCV/arch/riscv/generated/decoder-ns.hh.inc and you will see all the classes for different instructions. The execute method for these classes are implemented in build/RISCV/arch/riscv/generated/exec-ns.cc.inc . To know more on how these classes got built with gem5 please refer to my tutorial on [How to add instruction to RISCV ISA and simulate on gem5](). gem5 minor cpu model does not read the registers while being issued to the functional unit pipelines. Instead, the function unit pipelines are just to model the delays and all the execution is done by calling execute() method in commitInst() method in the execute unit.&lt;/p&gt;

&lt;h1 id=&#34;instruction-fetch&#34;&gt;Instruction Fetch&lt;/h1&gt;

&lt;p&gt;The important part of instruction fetch is virtual address translation. This is done by calling cpu.thread.itb.translateTiming(request). The TLB is implemented in srch/riscv/tlb.cc and calls thread.threadContext.process.pTable.translate(req). Process class is implemented in sim/process.hh and has pTable of Class PageTableBase implemented in mem/page_table.cc. The translate method tries to translate the virtual address to a physical address and then stores the physical address in req.paddr. translateTimming(req, translation) then calls translation-&amp;gt;finish() which calls the finish() method on the BaseTLB::Translation object in the fetch1 unit.&lt;/p&gt;

&lt;h1 id=&#34;instruction-decoding&#34;&gt;Instruction Decoding&lt;/h1&gt;

&lt;p&gt;The fetch2 stage has decoder which belongs to the class TheISA::Decoder. For RISCV ISA its implementation can be found in src/arch/riscv/decoder.hh. The Decoder has two important functions moreBytes(pc,inst) and decode(nextPC). moreBytes is used to copy the 32-bit instruction into the decoder so that when decode() is invoked it can decode the instruction. However, each decoder calls a GenericISA::BasicDecodeCache::decode() method. This method is implemented in src/arch/generic/decode_cache.cc. This looksup if it has already decoded this instruction in the decodePages and instMap cache structures, if it does not find it there it calls the decodeInst() method of the original decoder. This is a templated method in src/arch/isa_parser.py. Whenever gem5 is compiled for specific ISA for eg. RISCV build/RISCV/arch/riscv/generated/decode-method.cc.inc gets generated which has the decodeInst(machInst) method. It mostly does nothing, just sees the OPCODE and FUNC bits of the instruction and in a switch case create an object of that particular instruction by calling its constructor. All these instruction classes are defined in build/RISCV/arch/riscv/generated/decoder-ns.hh.inc and are derived from the StaticInst class. The constructors of the instructions&amp;rsquo; classes and the base StaticInst class initialize the members like number of source and destinations registers, the immediate value, machInst(actual instruction in binary). The constructors of these instruction classes are given in build/RISCV/arch/riscv/generated/decoder-ns.cc.inc with their class definitions in build/RISCV/arch/riscv/generated/decoder-ns.hh.inc. To create a dynamic instruction the machine instruction is passed to the decoder which creates a StaticInt object and returns it. This StaticInst object is then stick into the object of dynamic instruction.&lt;/p&gt;

&lt;h1 id=&#34;squashing-in-minor-cpu&#34;&gt;Squashing in minor cpu&lt;/h1&gt;

&lt;h1 id=&#34;function-units&#34;&gt;Function Units&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;// Here QeueuedInst is just a wrapper around MinorDynInst
typedef SelfStallingPipeline&amp;lt; QueuedInst, ReportTraitsAdaptor&amp;lt;QueuedInst&amp;gt; &amp;gt; FUPipelineBase;

class FUPipeline : public FUPipelineBase, public FuncUnit {
  LaneFU &amp;amp;description; 
  std::bitset&amp;lt;Num_OpClasses&amp;gt; capabilityList;  
}

clas LaneFU : public SimObject {
  Cycles opLat; // Latency
  Cycles issueLat; // Delay after issuing an operation before next is issued.
                   // normally 1 due to pipelining, but for divide unit as it 
                   // is not pipelined it is &amp;gt; 1

  std::vector&amp;lt;LaneFUTiming *&amp;gt; timings;  
}

class LaneFUTiming : public SimObject {
  Cycles extraCommitLat;
  Cycles extraAssumedLat; // extra delay to show in scoreboard after inst leaving
                          // the pipeline. Normally 0 but for mult it is 2

   
   /* for each of this instruction&#39;s source registers (in srcRegs
   *  order). The offsets are subtracted from scoreboard returnCycle times.
   *  For example, for an instruction type with 3 source registers,
   *  [2, 1, 2] will allow the instruction to issue upto 2 cycles early
   *  for dependencies on the 1st and 3rd register and upto 1 cycle
   *  early on the 2nd. */

  // ?? not so sure but if the list has a single number then latencies for
  // all the src regs is the same?? eg. for LaneDefaultIntFU it is [2]
  // and LaneDefaultIntMulFU it is [0]
  std::vector&amp;lt;Cycles&amp;gt; srcRegsRelativeLats; 
  
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;scoreboard&#34;&gt;ScoreBoard&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;class Scoreborad : public Named {
  std::vector&amp;lt;int&amp;gt; fuIndices; 

  // number of results which are not predictable ( memory loads in general )
  std::vector&amp;lt;Index&amp;gt; numUnpredictableResults;

  // estimated cycle number that the result will be presented.
  std::vector&amp;lt;Cycles&amp;gt; returnCycle;
}

// Called before issuing an instruction
void Scoreboard::markupInstDests(Cycles retire_time, bool mark_unpredictable ) {
  for ( dest_index = 0; dest_index &amp;lt; num_dests; dest_index++ ) {
    reg = flattenRegIndex(staticInst-&amp;gt;destRegIdx(dest_index));
    numResults[reg]++;
    if (mark_unpredictable)
      numUnpredictableResults[reg]++;
    returnCycle[reg] = retire_time; // retire_time is curCycle() + inst-&amp;gt;fu-&amp;gt;opLat
    fuIndices[reg] = inst-&amp;gt;fuIndex; // save the function unit id for this dest reg
  }
} 

bool Scoreboard::canInstIssue( std::vector&amp;lt;Cycles&amp;gt; *src_reg_relative_latencies,
                               // srcRegRelativeLats for that inst. see LaneFUTiming above
			       std::vector&amp;lt;bool&amp;gt; *cant_forward_from_fu_indices ) {

   for ( src_index = 0; src_index &amp;lt; inst-&amp;gt;numSrcRegs(); src_index++ ) {
     reg = flattenRegIndex(staticInst-&amp;gt;srcRegIdx(src_index));
     Cycles relative_latency = src_index &amp;gt;= src_reg_relative_latencies-&amp;gt;size()-1 ?
			         src_reg_relative_latencies-&amp;gt;back() // last element
			       : *(src_reg_relative_latencies)[src_index];

     if ( returnCycle[reg] &amp;gt; curCycle() + relativeLatency )
       return false;     
   }

   return true;
}
			      
void Scoreboard::clearInstDests(inst, bool clear_unpredictable) {

  for ( dest_index = 0; dest_index &amp;lt; num_dests; dest_index++ ) {
    RegIndex reg = inst-&amp;gt;flatDestRegIdx[dest_index];
    if ( clear_unpredictable )
      numUnpredictableResults[reg]--;

    numResults[reg]--;

    if (numResults[reg] == 0) {
      returnCycle[reg] = Cycles(0);
      writingInst[reg] = 0;
      fuIndices[reg] = -1;
    }
  }
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;load-store-queue-lsq&#34;&gt;Load-Store queue (LSQ)&lt;/h1&gt;

&lt;p&gt;Load store queue is used for performing load/store operations. It has three major hardware FIFOs: requests queue, transfers queue, and store buffer. Requests queue holds the memory requests which have been sent to the TLB but haven&amp;rsquo;t received the response for. Transfers queue holds the memory requests which have been issued to the memory system and are waiting for a response. When the responses arrive, the corresponding request is searched in the transfers queue (CAM like search) and its status is set to be Completed. These requests still remain in the transfers queue until the execute unit actually tries to commit the load/store instruction. When execute unit tries to commit load/store, it looks at the front of the transfers queue and pops out the element if the memory access was completed (see the execute unit description above). Store buffer has all the stores that have been committed but haven&amp;rsquo;t been written to memory yet. Before trying to issue a load from requests queue to memory, store buffer is checked to see if the previous store contains the data. The main top level function for LSQ is step()  which is called in the evaluate() method of the execute unit.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;LSQ::step() {
  tryToSendToTransfers(requests.front());
  storeBuffer.step()
}

LSQ::tryToSendToTransfers(LSQRequest* request) {
  // note that LSQRequest is not of type Request but a derived class 
  // from BaseTLB::Translation and Packet::SenderState
  LSQRequest request = requests.front();
  
  if (request-&amp;gt;isComplete()) {
    // LSQ::moveFromRequestsToTransfers(LSQRequestPtr request)
    requests.pop();
    transfers.push(request);
  }
  
  if (!(request-&amp;gt;isLoad &amp;amp;&amp;amp; storeBuffer.canForwardData())) {
    if (tryToSend(request)) {
      requests.pop();
      transfers.push(request);
    }
  }
}

LSQ::tryToSend(LSQRequest* request) {
  dacachePort.sendTimingRequest(request-&amp;gt;packet);
  // Set the state of the request appropriately depending on success or failure.
}

LSQ::recvTimingResp(Packet* response) {
  // the packet for memory requests contain LSQRequest pointers (can think of 
  // ids associated to each request that has been sent so far).
  // These LSQRequests are stored in transfers queue. When a memory response
  // is received the corresponding entry is searched in transfers queue and
  // its status is set to Complete. In real hardware this process should be
  // to search for the corresponding id location and then flip the status bit.
  // Search in a queue can be done using CAM based approach.
  response-&amp;gt;popSenderState()-&amp;gt;setStatus(Complete);
}

LSQ::StoreBuffer::step() {
  LSQRequest** i = slots.begin();
  while (issued &amp;amp;&amp;amp; issue_count &amp;lt; StoreLimit &amp;amp;&amp;amp; i!=slots.end()) {
    LSQRequest* request = *i;
    lsq.tryToSend(request);
  }
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Other important functions are pushRequest(): called by the exec_context.hh in initiateMemRead() function. pushRequest() calls&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;LSQ::pushRequest() {
  if (!isLoad)
    request_data = new uint8_t[size];
  
  LSQrequest* request = new SingleDataRequest();
  request-&amp;gt;request.setContext();
  request-&amp;gt;request.setVirt();
  requests.push(request);
  // request-&amp;gt;startAddrTranslation(); 
  thread-&amp;gt;getDTBPtr()-&amp;gt;translateTiming(request-&amp;gt;request);
  // TranslateTiming calls the finish() method when done with translation
}

LSQ::SingleDataRequest::finish() {
  makePacket();
  port.tryToSendToTransfers();
}

LSQ::LSQRequest::makePacket() {
  // packet = makePacketForRequest(request, data);
  Packet* packet = isLoad ? Packet::createRead(&amp;amp;request)
                          : Packet::createWrite(&amp;amp;request);

  packet-&amp;gt;pushSenderState(this);
  if (isLoad) packet-&amp;gt;allocate();
  else        packet-&amp;gt;dataDynamic(data);

  data = NULL;
}

&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;caches-in-gem5&#34;&gt;Caches in gem5&lt;/h1&gt;

&lt;p&gt;L1Cache, L1_ICache, L1_DCache, L2Cache, IOCache and  PageTableWalkerCache all are derived from class Cache by ovewriting the latency and size configurations. class Cache is defined in src/mem/cache/cache.hh and is the child class of BaseCache in src/mem/cache/base.hh.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;class BaseCache {
  // internal data structures
  MSHRQueue mshrQueue;
  WriteQueue writeBuffer;

  // parameters
  int blkSize;
  Cycles lookupLatency;
  Cycles dataLatency;
  Cycles forwardLatency;
  Cycles fillLatency;
  Cycles responseLatency;
  int numTargets;
};

class Cache : public BaseCache {
  // internal data structures
  BaseTags *tags;
  BasePrefetcher *prefetcher;
};

class BaseTags {
  int blkSize; // cache line size
               // since cache line size should be same for all cache levels
               // blkSize is a property of &amp;quot;system&amp;quot; and not just a cache.
               // hence this is set in config by &amp;quot;system.cache_line_size = ..&amp;quot;
               // and Tags.py copies that parameter into blkSkize variable 
  int size; // size of cache
  ...
};

/* There are many types of tag stores derived from BbaseTags
 * BaseTags -&amp;gt; BaseSetAssoc  ------&amp;gt; LRU
 *            (set associative |---&amp;gt; RandomRepl
 *                 caches)
 * BaseTags -&amp;gt; FALRU -&amp;gt; 
 *           (fully assoc.
 *               LRU)
 */
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which cache parameters we actually use is set in configs/common/Caches.py.
The specific tag store can either be set in common/caches/Caches.py. The
default tag store can be seen in src/mem/caches/Caches.py as LRU mostly.
Now, since LRU is most common, let&amp;rsquo;s understand LRU tag store&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;class BaseSetAssoc : public BaseTags {
  // data structures
  CacheSet&amp;lt;CacheBlk&amp;gt;* sets; // cache sets
  CacheBlk* blks; // cache blocks
  uint8_t* dataBlks; // data blocks (1 per cache block)

  // parameters
  int assoc; // associativity
  itn numSets;
};

class CacheBlk  {
  enum CacheBlkStatusBits {..., BlkDirty, ....};

  Addr tag; // data block tag value
  unit8_t* data; // contains copy of data for easy access
  int set, way; // set and ay this block belongs to
};

template &amp;lt;class BlkType&amp;gt;
class CacheSet {
  int assoc; // associativity of this set
  BlkType **blks; // cache blocks in this set
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The recvTimingReq() method for the non-blocking cache looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;Cache::recvTimingReq() {
  CacheBlk* blk = NULL; 
  std::list&amp;lt;PacketPtr&amp;gt; writebacks;
  bool satisfied = access(pkt, blk, writebacks);
  doWritebacks(writebacks);
  if (satisfied) {
    if (pkt-&amp;gt;needsResponse()) {
      pkt-&amp;gt;makeTimingResponse();
      cpuSidePort-&amp;gt;schedTimingResp(pkt, request_time);
    }
  }
}

Cache::access(pkt, blk, writebacks) {
  blk = tag-&amp;gt;accessBlock(pkt);
}

LRU::accessBlock(pkt, is_secure, &amp;amp;lat) { 
  // LRU class is derived from BaseSetAssoc class
  CacheBlk * blk = BaseSetAssoc::accessBlock(pkt-&amp;gt;getAddr(), is_secure, &amp;amp;lat);
  // move the block to the head of MRU
  sets[blk-&amp;gt;set].moveToHead(blk); 
  return blk;
}

BaseSetAssoc::accessBlock(addr, is_secure, &amp;amp;lat) {
  Addr tag = extractTag(addr);
  int set = extractSet(addr);
  blk = sets[set].findBlk(tag);
  lat = accessLatency;
}

template &amp;lt;class Blktype&amp;gt;
CacheSet&amp;lt;Blktype&amp;gt;::findBlk(tag, is_secure) {
  for (int i = 0; i &amp;lt; assoc; ++i) {
    if (blks[i]-&amp;gt;tag == tag &amp;amp;&amp;amp; blks[i]-&amp;gt;isValid() &amp;amp;&amp;amp;
      blks[i]-&amp;gt;isSecure() == is_secure) {
      way_id = i;
      return blks[i];
    }
  }
  return nullptr;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;function-units-1&#34;&gt;Function Units&lt;/h1&gt;

&lt;p&gt;The function units and what operations they can perform is described in MinorCPU.py by setting the variable &amp;ldquo;executeFuncUnits&amp;rdquo;. It by default uses MinorDefaultFUPool class which is defined in MinorCPU.py as well and creates a pool of function units (2: Int, 1: Int-mul, 1: int-div, 1: float-simd, 1: mem, 1: misc)&lt;/p&gt;

&lt;p&gt;Functional unit pipelines can be of variable depth, so this is modelled by the class FUPipeline which is derived from FUPipelineBase which is basically SelfStallingPipeline derived from TimeBuffer. The depth the FUPipelines are set in the Python file MinorCPU.py. The opLat parameter in MinorDefaultIntFU etc are the pipeline depths that get assigned to the depth of the SelfStalling pipelines.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Adding custom instruction to RISCV ISA and running it on gem5 and spike</title>
      <link>https://nitish2112.github.io/post/adding-instruction-riscv/</link>
      <pubDate>Mon, 10 Jul 2017 10:09:32 -0400</pubDate>
      
      <guid>https://nitish2112.github.io/post/adding-instruction-riscv/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;This is a tutorial on how to add an instruction to the RISCV ISA, how to write program with the special instruction. I will also talk about how to add the new instruction to RISCV assembler and how to execute it on gem5.&lt;/p&gt;

&lt;p&gt;First of all download and install the riscv-tools:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/riscv/riscv-tools.git
$ git submodule update --init --recursive
$ export RISCV=/path/to/install/riscv/toolchain
$ ./build.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will build the riscv toolchain. Now we will add a &amp;ldquo;modulo&amp;rdquo; instruction to the ISA. The instruction and its semantics are given below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mod r1, r2, r3

Semantics:
R[r1] = R[r2] % R[r3]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Open the file riscv-opcodes/opcodes, here you will be able to see the various opcodes and instruction bits assigned to various instructions. Assigned an unused instruction to modulo inst.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sra     rd rs1 rs2 31..25=32 14..12=5 6..2=0x0C 1..0=3
or      rd rs1 rs2 31..25=0  14..12=6 6..2=0x0C 1..0=3
and     rd rs1 rs2 31..25=0  14..12=7 6..2=0x0C 1..0=3

mod     rd rs1 rs2 31..25=1  14..12=0 6..2=0x1A 1..0=3

addiw   rd rs1 imm12            14..12=0 6..2=0x06 1..0=3
slliw   rd rs1 31..25=0  shamtw 14..12=1 6..2=0x06 1..0=3
srliw   rd rs1 31..25=0  shamtw 14..12=5 6..2=0x06 1..0=3
sraiw   rd rs1 31..25=32 shamtw 14..12=5 6..2=0x06 1..0=3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now run the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat opcodes-pseudo opcodes opcodes-rvc opcodes-rvc-pseudo opcodes-custom | ./parse-opcodes -c &amp;gt; ~/temp.h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Open file temp.h and you will find the following two lines:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;#define MATCH_MOD 0x200006b                                                    
#define MASK_MOD 0xfe00707f
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add these two lines in riscv-gnu-toolchain/riscv-binutils-gdb/include/opcode/riscv-opc.h&lt;/p&gt;

&lt;p&gt;Now edit riscv-gnu-toolchain/riscv-binutils-gdb/opcodes/riscv-opc.c and add this line in the assignment of const struct riscv_opcode riscv_opcodes[]:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;const struct riscv_opcode riscv_opcodes[] =                                     
{                                                                               
/* name,      isa,   operands, match, mask, match_func, pinfo.  */              
{&amp;quot;unimp&amp;quot;,     &amp;quot;C&amp;quot;,   &amp;quot;&amp;quot;,  0, 0xffffU,  match_opcode, 0 },                       
{&amp;quot;unimp&amp;quot;,     &amp;quot;I&amp;quot;,   &amp;quot;&amp;quot;,  MATCH_CSRRW | (CSR_CYCLE &amp;lt;&amp;lt; OP_SH_CSR), 0xffffffffU,  match_opcode, 0 }, /* csrw cycle, x0 */
{&amp;quot;ebreak&amp;quot;,    &amp;quot;C&amp;quot;,   &amp;quot;&amp;quot;,  MATCH_C_EBREAK, MASK_C_EBREAK, match_opcode, INSN_ALIAS },
{&amp;quot;ebreak&amp;quot;,    &amp;quot;I&amp;quot;,   &amp;quot;&amp;quot;,    MATCH_EBREAK, MASK_EBREAK, match_opcode, 0 },          
{&amp;quot;sbreak&amp;quot;,    &amp;quot;C&amp;quot;,   &amp;quot;&amp;quot;,  MATCH_C_EBREAK, MASK_C_EBREAK, match_opcode, INSN_ALIAS },
{&amp;quot;sbreak&amp;quot;,    &amp;quot;I&amp;quot;,   &amp;quot;&amp;quot;,    MATCH_EBREAK, MASK_EBREAK, match_opcode, INSN_ALIAS },
....
....
....
{&amp;quot;mod&amp;quot;,       &amp;quot;I&amp;quot;,   &amp;quot;d,s,t&amp;quot;,  MATCH_MOD, MASK_MOD, match_opcode, 0 }
....
....
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now compile the riscv-gnu-toolchain again and you are done.&lt;/p&gt;

&lt;p&gt;Lets write a program and see if the RISCV assembler can use the mod instruction:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;#include &amp;lt;stdio.h&amp;gt;

int main(){
  int a,b,c;
  a = 5;
  b = 2;
  asm volatile
  (
    &amp;quot;mod   %[z], %[x], %[y]\n\t&amp;quot;
    : [z] &amp;quot;=r&amp;quot; (c)
    : [x] &amp;quot;r&amp;quot; (a), [y] &amp;quot;r&amp;quot; (b)
  )  
 
  if ( c != 1 ){
     printf(&amp;quot;\n[[FAILED]]\n&amp;quot;);
     return -1;
  }
  
  printf(&amp;quot;\n[[PASSED]]\n&amp;quot;);

  return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compile it using the modified RISCV compiler&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ riscv64-unknown-elf-gcc mod.c -o mod
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now do an object dump to see that the mod instruction is actually used:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ riscv64-unknown-elf-objdump -dC mod &amp;gt; mod.dump
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now open mod.dump and you will be able to see that mod instruction has been used.&lt;/p&gt;

&lt;p&gt;I will later add how you can modify the ISA simulator to test the newly added instruction and how you can modify gem5 to be able to exexcute this instruction.&lt;/p&gt;

&lt;h1 id=&#34;adding-the-new-instruction-to-gem5&#34;&gt;Adding the new instruction to gem5&lt;/h1&gt;

&lt;p&gt;To add the instruction gem5 we need to modify arch/riscv/decoder.isa like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;    0x33: decode FUNCT3 {
        format ROp {
            0x0: decode FUNCT7 {
                0x0: add({{
                    Rd = Rs1_sd + Rs2_sd;
                }});
                0x1: mul({{
                    Rd = Rs1_sd*Rs2_sd;
                }}, IntMultOp);
                0x10: mod({{
                    Rd = Rs1_sd % Rs2_sd;
                }});
                0x20: sub({{
                    Rd = Rs1_sd - Rs2_sd;
                }});
            }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here 0x33 is the opcode and 0x10 is the the funct7. The details about different fields in the RISC-V ISA can be seen from page no. 50 of the manual &lt;a href=&#34;https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-54.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Note the instruction matching in the assembler is done something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-C&#34;&gt;((insn ^ op-&amp;gt;match) &amp;amp; op-&amp;gt;mask) == 0;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See match_opcode() fucntion in riscv-gnu-toolchain/riscv-binutils-gdb/opcodes/riscv-opc.c&lt;/p&gt;

&lt;p&gt;op-&amp;gt;match and op-&amp;gt;mask are the MATCH and MASK macros that we defined.&lt;/p&gt;

&lt;p&gt;This means if you just see the 1 bits in the MATCH in the instruction and flip them,
and then see the mask bits then everything should be 0. MASK tells which bits are
of interest and MATCH tells what is the configuration required.&lt;/p&gt;

&lt;h1 id=&#34;adding-the-custom-instruction-to-spike-isa-simulator&#34;&gt;Adding the custom instruction to spike ISA simulator&lt;/h1&gt;

&lt;p&gt;In the riscv-isa-sim/riscv/encoding.h add the following lines:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#define MATCH_MOD 0x200006b                                                    
#define MASK_MOD 0xfe00707f
...
DECLARE_INSN(mod, MATCH_MOD, MASK_MOD)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a file riscv-isa-sim/riscv/insns/mod.h and add these lines:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;WRITE_RD(sext_xlen(RS1 % RS2));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add this file to riscv-isa-sim/riscv/riscv.mk.in&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;riscv_insn_list = \
      ...
      mod \
      ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In riscv-isa-sim/spike_main/disasm.cc add the following lines:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DEFINE_RTYPE(mod);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now build riscv-tools again. The &amp;ldquo;mod&amp;rdquo; instruction has been added to spike
simulator&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Adding custom statistics to gem5</title>
      <link>https://nitish2112.github.io/post/statistics-gem5/</link>
      <pubDate>Mon, 10 Jul 2017 10:09:32 -0400</pubDate>
      
      <guid>https://nitish2112.github.io/post/statistics-gem5/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;This is a tutorial on how to add statistics in gem5.&lt;/p&gt;

&lt;p&gt;&amp;ndash; All the stats are defined in the namespace Stats. namespace Stats
   is spread across the following files:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;   -- src/base/statistics.hh
   -- src/sim/stat\_control.hh
   -- src/base/stats/info.hh
   -- src/base/stats/output.hh
   -- src/base/stats/text.hh
   -- src/base/stats/type.hh
   -- src/python/pybind11/stats.cc
   -- sim/power/mathexpr\_powermodel.hh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The schedStatEvent() function is used to dump statistics either all at once or
periodically. This is defined in src/sim/stat_control.cc. schedStatEvent()
creates a new StatEvent. The default values for when is curTick and repeat is
0, so the StatEvent created by schedStatEvent executes in the same cycle and
does not push itself to the event queue again when processed. The process()
function of StatEvent calls Stats::dump() and Stats::reset() both of which
are defined in base/statistics.hh. The dump() calls the constructor of
dumpHandler() which is a variable &amp;ldquo;Handler dumpHandler&amp;rdquo; in the Stats namespace
in the same file initialized to NULL. Same for reset and resetHandler(). The
dumpHandler and resetHandler variables are set using the call to registerHanlders()
also defined in the same file.&lt;/p&gt;

&lt;p&gt;When the gem5 system initantiates the objects by calling instantiate() method
in src/python/m5/simulate.py, in instantiate() method it calls stats.initSimStats()
which is defined in /src/python/m5/stats/&lt;strong&gt;init&lt;/strong&gt;.py and calls
_m5.stats.registerPythonStatsHandlers() which in turn is defined in
src/sim/stat_register.cc and calls registerHandlers(pythonReset, pythonDump);
to regisetr the handlers we were talking about in the previous paragraph.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h1 id=&#34;minor-cpu&#34;&gt;Minor CPU&lt;/h1&gt;</description>
    </item>
    
    <item>
      <title>Common tips for technical writing</title>
      <link>https://nitish2112.github.io/post/technical-writing/</link>
      <pubDate>Mon, 10 Jul 2017 10:09:32 -0400</pubDate>
      
      <guid>https://nitish2112.github.io/post/technical-writing/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;comma-splice&#34;&gt;&lt;strong&gt;Comma Splice&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;A comma splice is a grammatical error in which two independent clauses are joined by only a comma.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;INCORRECT&lt;/strong&gt;: It was 500 miles to the facility, we arranged to fly.&lt;/p&gt;

&lt;p&gt;A comma splice can be corrected in several ways.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Substitute a semicolon, a semicolon and a conjunctive adverb, or a comma and a coordinating conjunction.&lt;br /&gt;
&amp;ndash; It was 500 miles to the facility; we arranged to fly.&lt;br /&gt;
&amp;ndash; It was 500 miles to the facility; therefore, we arranged to fly.&lt;br /&gt;
&amp;ndash; It was 500 miles to the facility, so we arranged to fly.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create two sentences.&lt;br /&gt;
&amp;ndash; It was 500 miles to the facility. We arranged to fly.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Subordinate one clause to the other.&lt;br /&gt;
&amp;ndash; Because it was 500 miles to the facility, we arranged to fly.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;a-linking-independent-clauses&#34;&gt;A. Linking Independent Clauses&lt;/h2&gt;

&lt;p&gt;Use a comma before a coordinating conjunction (and, but, or, nor, and sometimes so, yet, and for) that links independent clauses.&lt;/p&gt;

&lt;p&gt;E.g. The new microwave disinfection system was delivered, but the installation will require an additional week.&lt;/p&gt;

&lt;p&gt;However, if two independent clauses are short and closely related—and there is no danger of confusing the reader—the comma may be omitted.
Both of the following examples are correct.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CORRECT&lt;/strong&gt;: The cable snapped and the power failed.&lt;br /&gt;
&lt;strong&gt;CORRECT&lt;/strong&gt;: The cable snapped, and the power failed.&lt;/p&gt;

&lt;h2 id=&#34;b-enclosing-elements&#34;&gt;B. Enclosing Elements&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1) Commas are used to enclose nonessential information (parenthetical elements)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CORRECT&lt;/strong&gt;: Our new factory, which began operations last month, should add
25 percent to total output.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CORRECT&lt;/strong&gt;: The technician, working quickly and efficiently, finished early.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CORRECT&lt;/strong&gt;: We can, of course, expect their lawyer to call us.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2) Yes and no are set off by commas in such uses as the following:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CORRECT&lt;/strong&gt;: No, I do not think we can finish by the deadline.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3) A direct address should be enclosed in commas.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CORRECT&lt;/strong&gt;: You will note, Jeff, that the surface of the brake shoe complies
with the specification.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4) An appositive phrase (which re-identifies another expression in the sentence) is enclosed in commas.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CORRECT:&lt;/strong&gt; Our company, Envirex Medical Systems, won several awards last year.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5) Interrupting parenthetical and transitional words or phrases are usually set off with commas.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CORRECT:&lt;/strong&gt; The report, therefore, needs to be revised.&lt;/p&gt;

&lt;p&gt;Commas are omitted when the word or phrase does not interrupt the continuity of thought.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CORRECT:&lt;/strong&gt; I therefore suggest that we begin construction.&lt;/p&gt;

&lt;h2 id=&#34;c-introducing-elements&#34;&gt;C. Introducing Elements&lt;/h2&gt;

&lt;h3 id=&#34;c-0-clauses-and-phrases&#34;&gt;C.0 Clauses and Phrases.&lt;/h3&gt;

&lt;p&gt;Generally, place a comma after an introductory clause or phrase, especially if it is long,
to identify where the introductory element ends and the main part of the sentence begins.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CORRECT:&lt;/strong&gt; Because we have not yet contained the new strain of influenza, we recommend vaccination for high-risk patients.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1) A long modifying phrase that precedes the main clause should always be followed by a comma.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;During the first series of field-performance tests at our Colorado proving ground, the new engine failed to meet our expectations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2) When an introductory phrase is short and closely related to the main clause, the comma may be omitted.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In two seconds a 5F temperature rise occurs in the test tube.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3) A comma should always follow an absolute phrase, which modifies the whole sentence.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The tests completed, we organized the data for the final report.&lt;/p&gt;

&lt;h3 id=&#34;c-1-words-and-quotations&#34;&gt;C.1 Words and Quotations.&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1) Certain types of introductory words are followed by a comma. One example is a transitional word or phrase
(however, in addition) that connects the preceding clause or sentence with the thought that follows.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Furthermore, steel can withstand a humidity of 99 percent, provided that there is no chloride or sulfur dioxide in the atmosphere.&lt;/p&gt;

&lt;p&gt;For example, this change will make us more competitive in the global marketplace.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2) When an adverb closely modifies the verb or the entire sentence, it should not be followed by a comma.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Perhaps we can still solve the turnover problem. Certainly we should try. [Perhaps and certainly closely modify each statement.]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3) A proper noun used in an introductory direct address is followed by a comma, as is an interjection (such as oh, well, why, indeed, yes, and no).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Nancy, enclosed is the article you asked me to review. [direct address]&lt;/p&gt;

&lt;p&gt;Indeed, I will ensure that your request is forwarded. [interjection]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4) Use a comma to separate a direct quotation from its introduction.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Morton and Lucia White said, &amp;ldquo;People live in cities but dream of the countryside.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5) Do not use a comma when giving an indirect quotation.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Morton and Lucia White said that people dream of the countryside, even though they live in cities.&lt;/p&gt;

&lt;h2 id=&#34;d-separating-items-in-a-series&#34;&gt;D. Separating Items in a Series&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1) Although the comma before the last item in a series is sometimes omitted, it is generally clearer to include it.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Random House, Bantam, Doubleday, and Dell were individual publishing companies.
[Without the final comma, “Doubleday and Dell” might refer to one company or two.]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2) Phrases and clauses in coordinate series are also punctuated with commas.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Plants absorb noxious gases, act as receptors of dirt particles, and cleanse the air of other impurities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3) When phrases or clauses in a series contain commas, use semicolons rather than commas to separate the items.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our new products include amitriptyline, which has sold very well; and cholestyramine, which was just introduced.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4) When adjectives modifying the same noun can be reversed and make sense, or when they can be separated by and or or, they should be separated
by commas.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The aircraft featured a modern, sleek, swept-wing design.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5) When an adjective modifies a phrase, no comma is required.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;She was investigating the damaged inventory-control system. [The adjective damaged modifies the phrase inventory-control
system.]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6) Never separate a final adjective from its noun.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;He is a conscientious, honest, reliable[, not needed here] worker.&lt;/p&gt;

&lt;h2 id=&#34;e-clarifying-and-contrasting&#34;&gt;E. Clarifying and Contrasting&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1) Use a comma to separate two contrasting thoughts or ideas.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The project was finished on time, but not within the budget.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2) Use a comma after an independent clause that is only loosely related to the dependent clause that follows it or that could be misread without the comma.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I should be able to finish the plan by July, even though I lost time because of illness.&lt;/p&gt;

&lt;h2 id=&#34;f-showing-omissions&#34;&gt;F. Showing Omissions&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1) A comma sometimes replaces a verb in certain elliptical constructions.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Some were punctual; others, late. [The comma replaces were.]&lt;br /&gt;
It is better, however, to avoid such constructions in technical writing.&lt;/p&gt;

&lt;h2 id=&#34;e-using-with-numbers-and-names&#34;&gt;E. Using with Numbers and Names&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1)  Commas are conventionally used to separate distinct items. Use commas between the elements of an address written on the same line (but not between the state and the ZIP Code).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kristen James, 4119 Mill Road, Dayton, Ohio 45401&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2) A full date that is written in month-day-year format uses a comma preceding and following the year.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;November 30, 2020, is the payoff date.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3) Do not use commas for dates in the day-month-year format, which is used in many parts of the world and by the U.S. military. See also international
correspondence.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Note that 30 November 2020 is the payoff date.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4) No commas are used when showing only the month and year or month and day in a date.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The target date of May 2012 is optimistic, so I would like to meet on March 4 to discuss our options.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5) Use commas to separate the elements of Arabic numbers.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1,528,200 feet&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6) Use commas to separate the elements of geographical names.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Toronto, Ontario, Canada&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7) Use a comma to separate names that are reversed (Smith, Alvin) and commas with professional abbreviations.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Jim Rogers Jr., M.D., chaired the conference.
[ Jr. or Sr. does not require a comma.]&lt;/p&gt;

&lt;h2 id=&#34;f-using-with-other-punctuation&#34;&gt;F. Using with Other Punctuation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1) Conjunctive adverbs (however, nevertheless, consequently, for example, on the other hand) that join independent clauses are preceded by a semicolon and followed by a comma. Such adverbs function both as modifiers and as connectives.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The idea is good; however, our budget is not sufficient.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2) When a comma should follow a phrase or clause that ends with words in parentheses, the comma always appears outside the closing parenthesis.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Although we left late (at 7:30 p.m.), we arrived in time for the keynote address.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3) Commas always go inside quotation marks.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The operator placed the discharge bypass switch at “normal,” which triggered a second discharge.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4) Except with abbreviations, a comma should not be used with a dash, an exclamation mark, a period, or a question mark.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;“Have you finished the project?[no , here]” she asked.&lt;/p&gt;

&lt;h2 id=&#34;g-avoiding-unnecessary-commas&#34;&gt;G. Avoiding Unnecessary Commas&lt;/h2&gt;

&lt;p&gt;A number of common writing errors involve placing commas where
they do not belong. As stated earlier, such errors often occur because
writers assume that a pause in a sentence should be indicated by a
comma.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1) Do not place a comma between a subject and verb or between a verb and its object.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The conditions at the test site in the Arctic[no , here] made accurate readings
difficult.&lt;/p&gt;

&lt;p&gt;She has often said[no , here] that one company’s failure is another’s
opportunity.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2) Do not use a comma between the elements of a compound subject or compound predicate consisting of only two elements.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The director of the design department[no , here] and the supervisor of the quality-control section were opposed to the new schedules.&lt;/p&gt;

&lt;p&gt;The design director listed five major objections[no , here] and asked that the new schedule be reconsidered.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3) Do not include a comma after a coordinating conjunction such as and or but.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The chairperson formally adjourned the meeting, but[no , here] the members of the committee continued to argue.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4) Do not place a comma before the first item or after the last item of a series.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The new products we are considering include[no , here] calculators, scanners, and cameras.&lt;/p&gt;

&lt;p&gt;It was a fast, simple, inexpensive[no , here] process.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5) Do not use a comma to separate a prepositional phrase from the rest of the sentence unnecessarily.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We discussed the final report[no , here] on the new project.&lt;/p&gt;

&lt;h1 id=&#34;hyphen&#34;&gt;Hyphen&lt;/h1&gt;

&lt;p&gt;The hyphen (-) is used primarily for linking and separating words and parts of words. The hyphen often improves the clarity of writing. The
hyphen is sometimes confused with the dash, which may be indicated with two consecutive hyphens.&lt;/p&gt;

&lt;h2 id=&#34;hyphens-with-compound-words&#34;&gt;Hyphens with Compound Words&lt;/h2&gt;

&lt;p&gt;Some compound words are formed with hyphens (able-bodied, over-thecounter). Hyphens are also used with multiword numbers from twentyone
through ninety-nine and fractions when they are written out (threequarters). Most current dictionaries indicate whether compound words
are hyphenated, written as one word, or written as separate words.&lt;/p&gt;

&lt;p&gt;NOTE: These are the notes taken from Alred, Gerald J., Charles T. Brusaw, and Walter E. Oliu. Handbook of technical writing. Macmillan, 2009.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Gem5 Simulation Framework</title>
      <link>https://nitish2112.github.io/post/gem5-simulation/</link>
      <pubDate>Mon, 10 Jul 2017 10:09:32 -0400</pubDate>
      
      <guid>https://nitish2112.github.io/post/gem5-simulation/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;p&gt;This is an introduction tutorial to gem5 simulation framework&lt;/p&gt;

&lt;h1 id=&#34;simobject-in-gem5&#34;&gt;SimObject in gem5&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;/* src/sim/eventq.hh */
class EventManager {
  EventQueue * eventq;

  void schedule (Event &amp;amp;event, Tick when) { 
    eventq-&amp;gt;schedule(&amp;amp;event);
  }
};

/* src/sim/sim_object.hh */
class SimObject : public EventManager {
  void init();
  void startup();
}

/* src/sim/eventq.hh */
EventQueue* getEventQueue(int index) {
  while(numMainEventQueues &amp;lt;= index) {
    numMainEventQueues++;
    mainEventQueue.push_back(new EventQueue(index));
  }
}

/* src/sim/sim_object.cc */
SimObject::SimObject(Params *p) : EventManager(getEventQueue(p-&amp;gt;eventq_index)) {}

&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;creating-a-simobject&#34;&gt;Creating a SimObject&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;class VVADDXcel : SimObject {
  class VVADDXcelEvent : public Event {
    VVADDXcel *vvadd;
    void process() {
      if (!this-&amp;gt;scheduled()) {
        vvadd-&amp;gt;schedule(this, vvadd-&amp;gt;clockEdge(1));
      }
      vvadd-&amp;gt;tick();
    }
  }

  void tick() {
    /* Write the logic for posedge clock here */
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;simulation-using-gem5&#34;&gt;Simulation using gem5&lt;/h1&gt;

&lt;p&gt;The configuration file se.py calls Simulation.run(). The Simulation.py in config/common/Simulation.py implements the run() method. This method calls m5.instantiate() and then calls benchCheckpoints() methods. benchCheckpoints() method calls m5.simulate(). The m5 package is located in src/python/m5. m5.initantiate() and m5.simulate() both are implemented in simulate.py. The m5.instantiate() first gets the root instance by calling objects.Root.getInstance(). The Root package is in src/sim/Root.py. m5.simulate() then calls obj.createCCObject(), obj.connectPorts(), obj.init(), obj.regStats(), obj.regProbePoints(), obj.regProbeListeners(), and obj.initState() in sequence on all the obj which are the descendants of the root ( i.e. all the objects ). Each of these methods is defined in the C++ or python version of SimObject and can be reimplemented by the class itself. The m5.simulate() calls the startup() method on all the descendants of the root. The descendants of the root are basically all the objects in the tree of hierarchies not just the ones that are specified in the config file.  These startup methods are implemented in the classes for e.g. there is a startup method in class MinorCPU, BaseCPU, SimpleThread etc. After calling the startup() methods it calls _m5.event.simulate(). Don&amp;rsquo;t know how but this simulate() method calls simulate() method in src/sim/simulate.cc. The simulate method calls a doSimLoop() method which has a while(1) loop that calls eventq-&amp;gt;serviceOne() each iteration and the serviceOne method pops an event at the head of the queue and calls process() method of that event.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Gem5 miscellaneous</title>
      <link>https://nitish2112.github.io/post/gem5-misc/</link>
      <pubDate>Mon, 10 Jul 2017 10:09:32 -0400</pubDate>
      
      <guid>https://nitish2112.github.io/post/gem5-misc/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;adding-an-option-to-gem5&#34;&gt;Adding an option to gem5&lt;/h1&gt;

&lt;p&gt;Say we want to add option &amp;ldquo;xyz&amp;rdquo; to our simulator&lt;/p&gt;

&lt;p&gt;1) Add it to configs/common/Options.py: In addCommonOptions() function add
   parser.add_option(&amp;ldquo;&amp;ndash;xyz&amp;rdquo;, action=&amp;ldquo;store_true&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;2) Add this option as a parameter in base cpu class. In src/cpu/BaseCPU.py
   add this to the class BaseCPU:&lt;/p&gt;

&lt;p&gt;xyz = Param.Bool(False, &amp;ldquo;Enable line trace&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;3) Add xyz as a public class member in base class of cpu in src/cpu/base.hh&lt;/p&gt;

&lt;p&gt;bool xyz;&lt;/p&gt;

&lt;p&gt;4) Add support for xyz in constructor of base class for cpu in src/cpu/base.cc&lt;/p&gt;

&lt;p&gt;xyz               =  params()-&amp;gt;xyz;&lt;/p&gt;

&lt;p&gt;5) Pass the option from the config script to the cpu class. After instantiating
   cpus i.e.&lt;/p&gt;

&lt;p&gt;system = System(cpu = [CPUClass(cpu_id=i, num_cores=np)
                          for i in xrange(np)] &amp;hellip;. )&lt;/p&gt;

&lt;p&gt;etc, do&lt;/p&gt;

&lt;p&gt;system.cpu[0].xyz = options.xyz&lt;/p&gt;

&lt;p&gt;6) Now you can use the true/false boolean xyz to do conditional operation in
   you cpu member functions as:&lt;/p&gt;

&lt;p&gt;if ( xyz ) {  &amp;hellip;. } else { &amp;hellip; }&lt;/p&gt;

&lt;p&gt;We have now added a new option to gem5.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Accelerating Face Detection on Programmable SoC Using C-Based Synthesis</title>
      <link>https://nitish2112.github.io/publication/face-detect/</link>
      <pubDate>Fri, 10 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://nitish2112.github.io/publication/face-detect/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Flexible and Dynamic Power Allocation in Broadband Multi-Beam Satellites</title>
      <link>https://nitish2112.github.io/publication/dynamic-power-satellite/</link>
      <pubDate>Fri, 10 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://nitish2112.github.io/publication/dynamic-power-satellite/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pointer-Chase Prefetcher</title>
      <link>https://nitish2112.github.io/project/prefetcher/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://nitish2112.github.io/project/prefetcher/</guid>
      <description>

&lt;h1 id=&#34;caches-only-exploit-spatial-and-temporal-locality-in-a-set-of-address-referenced-in-a-program-due-to-dynamic-construction-of-linked-data-structures-they-difficult-to-cache-as-the-spatial-locality-between-the-nodes-is-highly-dependent-on-the-data-layout-prefetching-can-play-an-important-role-in-improving-the-performance-of-linked-data-structures-in-this-project-a-pointer-chase-mechanism-along-with-compiler-hints-is-adopted-for-prefetching-here-is-the-link-for-the-report-link-prefetcher-report-pdf&#34;&gt;Caches only exploit spatial and temporal locality in a set of address referenced in a program. Due to dynamic construction of linked data-structures, they difficult to cache as the spatial locality between the nodes is highly dependent on the data layout. Prefetching can play an important role in improving the performance of linked data-structures. In this project a pointer chase mechanism along with compiler hints is adopted for prefetching. Here is the link for the report &lt;a href=&#34;../prefetcher-report.pdf&#34;&gt;link&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;+++&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

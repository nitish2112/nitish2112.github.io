<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nitish Srivastava on Nitish Srivastava</title>
    <link>https://nitish2112.github.io/index.xml</link>
    <description>Recent content in Nitish Srivastava on Nitish Srivastava</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Nitish Srivastava</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Sketches</title>
      <link>https://nitish2112.github.io/personal/sketch-gallery/</link>
      <pubDate>Fri, 14 Jul 2017 11:40:57 -0400</pubDate>
      
      <guid>https://nitish2112.github.io/personal/sketch-gallery/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;div id=&#34;carousel-example-generic&#34; class=&#34;carousel slide&#34; data-ride=&#34;carousel&#34;&gt;
  &lt;!-- Indicators --&gt;
  &lt;ol class=&#34;carousel-indicators&#34;&gt;
    &lt;li data-target=&#34;#carousel-example-generic&#34; data-slide-to=&#34;0&#34; class=&#34;active&#34;&gt;&lt;/li&gt;
    &lt;li data-target=&#34;#carousel-example-generic&#34; data-slide-to=&#34;1&#34;&gt;&lt;/li&gt;
    &lt;li data-target=&#34;#carousel-example-generic&#34; data-slide-to=&#34;2&#34;&gt;&lt;/li&gt;
  &lt;/ol&gt;

  &lt;!-- Wrapper for slides --&gt;
  &lt;div class=&#34;carousel-inner&#34; role=&#34;listbox&#34;&gt;
    &lt;div class=&#34;item active&#34;&gt;
      &lt;img src=&#34;1.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item&#34;&gt;
      &lt;img src=&#34;2.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item active&#34;&gt;
      &lt;img src=&#34;3.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item&#34;&gt;
      &lt;img src=&#34;4.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item active&#34;&gt;
      &lt;img src=&#34;5.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item&#34;&gt;
      &lt;img src=&#34;6.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item active&#34;&gt;
      &lt;img src=&#34;7.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item&#34;&gt;
      &lt;img src=&#34;8.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item active&#34;&gt;
      &lt;img src=&#34;9.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item&#34;&gt;
      &lt;img src=&#34;10.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    ...
  &lt;/div&gt;

&lt;p&gt;&lt;!-- Controls --&gt;
  &lt;a class=&#34;left carousel-control&#34; href=&#34;#carousel-example-generic&#34; role=&#34;button&#34; data-slide=&#34;prev&#34;&gt;
    &lt;span class=&#34;glyphicon glyphicon-chevron-left&#34; aria-hidden=&#34;true&#34;&gt;&lt;/span&gt;
    &lt;span class=&#34;sr-only&#34;&gt;Previous&lt;/span&gt;
  &lt;/a&gt;
  &lt;a class=&#34;right carousel-control&#34; href=&#34;#carousel-example-generic&#34; role=&#34;button&#34; data-slide=&#34;next&#34;&gt;
    &lt;span class=&#34;glyphicon glyphicon-chevron-right&#34; aria-hidden=&#34;true&#34;&gt;&lt;/span&gt;
    &lt;span class=&#34;sr-only&#34;&gt;Next&lt;/span&gt;
  &lt;/a&gt;
&lt;/div&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Sketches</title>
      <link>https://nitish2112.github.io/post/sketch-gallery/</link>
      <pubDate>Fri, 14 Jul 2017 11:40:57 -0400</pubDate>
      
      <guid>https://nitish2112.github.io/post/sketch-gallery/</guid>
      <description>&lt;p&gt;&lt;/p&gt;

&lt;div id=&#34;carousel-example-generic&#34; class=&#34;carousel slide&#34; data-ride=&#34;carousel&#34;&gt;
  &lt;!-- Indicators --&gt;
  &lt;ol class=&#34;carousel-indicators&#34;&gt;
    &lt;li data-target=&#34;#carousel-example-generic&#34; data-slide-to=&#34;0&#34; class=&#34;active&#34;&gt;&lt;/li&gt;
    &lt;li data-target=&#34;#carousel-example-generic&#34; data-slide-to=&#34;1&#34;&gt;&lt;/li&gt;
    &lt;li data-target=&#34;#carousel-example-generic&#34; data-slide-to=&#34;2&#34;&gt;&lt;/li&gt;
  &lt;/ol&gt;

  &lt;!-- Wrapper for slides --&gt;
  &lt;div class=&#34;carousel-inner&#34; role=&#34;listbox&#34;&gt;
    &lt;div class=&#34;item active&#34;&gt;
      &lt;img src=&#34;1.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item active&#34;&gt;
      &lt;img src=&#34;2.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item active&#34;&gt;
      &lt;img src=&#34;4.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item active&#34;&gt;
      &lt;img src=&#34;6.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item active&#34;&gt;
      &lt;img src=&#34;7.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item active&#34;&gt;
      &lt;img src=&#34;8.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item active&#34;&gt;
      &lt;img src=&#34;9.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;item&#34;&gt;
      &lt;img src=&#34;10.jpg&#34; height=&#34;1080&#34; width=&#34;1080&#34; alt=&#34;...&#34;&gt;
      &lt;div class=&#34;carousel-caption&#34;&gt;
        ...
      &lt;/div&gt;
    &lt;/div&gt;
    ...
  &lt;/div&gt;

&lt;p&gt;&lt;!-- Controls --&gt;
  &lt;a class=&#34;left carousel-control&#34; href=&#34;#carousel-example-generic&#34; role=&#34;button&#34; data-slide=&#34;prev&#34;&gt;
    &lt;span class=&#34;glyphicon glyphicon-chevron-left&#34; aria-hidden=&#34;true&#34;&gt;&lt;/span&gt;
    &lt;span class=&#34;sr-only&#34;&gt;Previous&lt;/span&gt;
  &lt;/a&gt;
  &lt;a class=&#34;right carousel-control&#34; href=&#34;#carousel-example-generic&#34; role=&#34;button&#34; data-slide=&#34;next&#34;&gt;
    &lt;span class=&#34;glyphicon glyphicon-chevron-right&#34; aria-hidden=&#34;true&#34;&gt;&lt;/span&gt;
    &lt;span class=&#34;sr-only&#34;&gt;Next&lt;/span&gt;
  &lt;/a&gt;
&lt;/div&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Tutorial on the Gem5 Minor CPU Model</title>
      <link>https://nitish2112.github.io/post/adding-instruction-riscv/</link>
      <pubDate>Mon, 10 Jul 2017 10:09:32 -0400</pubDate>
      
      <guid>https://nitish2112.github.io/post/adding-instruction-riscv/</guid>
      <description>&lt;p&gt;This is an introduction tutorial on gem5 minor cpu model.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Tutorial on the Gem5 Minor CPU Model</title>
      <link>https://nitish2112.github.io/post/gem5-minor-cpu/</link>
      <pubDate>Mon, 10 Jul 2017 10:09:32 -0400</pubDate>
      
      <guid>https://nitish2112.github.io/post/gem5-minor-cpu/</guid>
      <description>&lt;p&gt;This is an introduction tutorial on gem5 minor cpu model.&lt;/p&gt;

&lt;p&gt;Many a times it gets difficult for the computer architects to get started with event-driven simulators. This document is written to target that audience and provide an overview of the minor cpu model in gem5 which implements an in-order pipelined processor. If you have never worked on event-driven simulators and don&amp;rsquo;t know what they are, there is a cool video &lt;a href=&#34;https://www.youtube.com/watch?v=irbshkdVFao&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;. This tutorial will help the reader to understand how the event-driven minor cpu model is implemented in gem5 and will not go much into details of how to compile and build gem5, how to add tracing and what are ports and how do they work. This information can be found in &lt;a href=&#34;http://learning.gem5.org/book/index.html&#34; target=&#34;_blank&#34;&gt;Learning Gem5&lt;/a&gt;. OK!! So lets get started.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;minor-cpu&#34;&gt;Minor CPU&lt;/h1&gt;

&lt;h2 id=&#34;pipeline&#34;&gt;Pipeline&lt;/h2&gt;

&lt;p&gt;It is a 4 stage pipelined processor. The four stages being fetch1, fetch2, decode and execute. As opposed to 5-stage DLX pipeline that every computer architecture student is familiar with this is somewhat different. The ITLB access, and fetch of the instruction from the main memory are done in fetch1. fetch2 is responsible for decoding the instruction, decode is responsible for just some book-keeping ( why this is a stage I am not sure at this point ) and execute implements the logic for issue, execute, memory, writeback and commit. All of these stages are defined as SimObjects in the class Pipeline which implements the entire pipeline. The different pipeline stages are connected via Latches ( we will talk about their implementation later in this tutorial ).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;class Pipeline {
    /* Latches to connect the stages */
    Latch&amp;lt;ForwardLineData&amp;gt; f1ToF2;  
    Latch&amp;lt;BranchData&amp;gt; f2ToF1;
    Latch&amp;lt;ForwardInstData&amp;gt; f2ToD;
    Latch&amp;lt;ForwardInstData&amp;gt; dToE;
    Latch&amp;lt;BranchData&amp;gt; eToF1;

    /* Pipeline Stages */
    Execute execute; 
    Decode decode;     
    Fetch2 fetch2;
    Fetch1 fetch1

    /* Action to be performed at each cycle (tick) */
    void evaluate();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;execute&#34;&gt;Execute&lt;/h2&gt;

&lt;p&gt;The way to think about an object/class is in terms of its data members as they correspond to the physical data-structures that you will build in your hardware. The methods tell how these objects interact which somewhat represents the wiring and the control. The main objects in the Execute stage are shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;class Execute {
    Latch&amp;lt;ForwardInstData&amp;gt;::Output inp; // connected to dToE Latch
    Latch&amp;lt;BranchData&amp;gt;::Input out; // connected to eToF1 Latch
    /** Scoreboard of instruction dependencies */
    std::vector&amp;lt;Scoreboard&amp;gt; scoreboard;
    /** The execution functional units */
    std::vector&amp;lt;FUPipeline *&amp;gt; funcUnits;
    std::vector&amp;lt;InputBuffer&amp;lt;ForwardInstData&amp;gt;&amp;gt; inputBuffer;
    void evaluate();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then there is an evaluate() method which is the actiopn this stage has to perform at each CPU tick. As it can be seen from the implementation of Execute class that there are two wires one for input and one for output. The one named &amp;ldquo;inp&amp;rdquo; is connected to the output of dTE Latch and the one named &amp;ldquo;out&amp;rdquo; is connected to eToF1 Latch by the constructor of Pipeline class. The dToE Latch carries the instructions from decode to execute and eToF1 carries branch updates (the branch outcome is known only in execute) to the fetch1 stage. There is vector of objects of the class Scoreboard. Each element in the vector corresponds to a scoreboard for a thread. As the processor can be multithreaded this is necessary to seperate the scoreboards of different threads. In the rest of this tutorial I will assume a single threaded processor, so only scoreboard[0] will be a valid entry. Same is the case for the funcUnits which is a vector of functional unit pipelines for different threads and inputBuffer. The figure below gives a pictorial representation of how the execute stage looks like.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;gem5-logo.png&#34; alt=&#34;gem5&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;decode&#34;&gt;Decode&lt;/h2&gt;

&lt;p&gt;The important data-structures and methods in the decode class are shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;class Decode {
    Latch&amp;lt;ForwardInstData&amp;gt;::Output inp; // connected to f2ToD
    Latch&amp;lt;ForwardInstData&amp;gt;::Input out;  // connected to dToE
    /* references to execute.inputBuffer vector */
    std::vector&amp;lt;InputBuffer&amp;lt;ForwardInstData&amp;gt;&amp;gt; &amp;amp;nextStageReserve;  
    std::vector&amp;lt;InputBuffer&amp;lt;ForwardInstData&amp;gt;&amp;gt; inputBuffer;
    std::vector&amp;lt;DecodeThreadInfo&amp;gt; decodeInfo;
    void evaluate();

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Similar to execute stage, decode contains one input wire and one output wire, but this time the input wire is carrying the decoded instruction from fetch2 to decode and the output wire is carrying the output instruction from decode to execute. As decode does not determine the outcome of branches there is no wire going from decode to fetch1 as in the case of execute. decode also contains an alias ( reference ) to the inputBuffer object of the execute stage. This reference is used to reserve an entry in the inputBuffer of the execute stage. By doing this decode makes sure that whatever instruction it is inserting in the dTE latch will have a place in the input buffer of the execute from which it executes the instructions.&lt;/p&gt;

&lt;p&gt;&amp;hellip; goes here
&lt;img src=&#34;gem5-logo.png&#34; alt=&#34;gem5&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;fetch2&#34;&gt;Fetch2&lt;/h2&gt;

&lt;p&gt;The important data-structures and methods in fetch2 class are shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;class Fetch2 {
  Latch&amp;lt;ForwardLineData&amp;gt;::Output inp;  // connected to f1ToF2
  Latch&amp;lt;BranchData&amp;gt;::Output branchInp; // connected to eToF1
  Latch&amp;lt;BranchData&amp;gt;::Input predictionOut; // coneected to f2ToF1
  Latch&amp;lt;ForwardInstData&amp;gt;::Input out;      // connected to f2ToD
  
  std::vector&amp;lt;InputBuffer&amp;lt;ForwardInstData&amp;gt;&amp;gt; &amp;amp;nextStageReserve
  std::vector&amp;lt;InputBuffer&amp;lt;ForwardLineData&amp;gt;&amp;gt; inputBuffer;
  std::vector&amp;lt;Fetch2ThreadInfo&amp;gt; fetchInfo;

  BPredUnit &amp;amp;branchPredictor;   // Branch Predictor

  void evaluate();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By now it should be clear what are the different components in the pipeline stages are. First of all there are some wires which are either input to the stage (and hence the Output of the Latch) or are outputs from the stage (Inputs to the Latches). Then there is an instruction buffer for each stage which holds entries of ForwardInstData or ForwardLine data which are just instructions wrapped in a class. Then at last there is a reference to the input buffer of the next stage and an evaluate() function which gets executed every clock tick. The main function that fetch2 is responsible for is decoding the instruction. The names can be sometimes misleading as the decode stage is not the one that does the real decoding. The decoding is indeed done in the fetch2 stage.&lt;/p&gt;

&lt;h2 id=&#34;fetch1&#34;&gt;Fetch1&lt;/h2&gt;

&lt;p&gt;Not being so verbose fetch1 looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;class Fetch1 {
  Latch&amp;lt;BranchData&amp;gt;::Output inp;        // connected to eToF1
  Latch&amp;lt;ForwardLineData&amp;gt;::Input out;    // connected to f1ToF2
  Latch&amp;lt;BranchData&amp;gt;::Output prediction; // connected to f2ToF1
  
  std::vector&amp;lt;InputBuffer&amp;lt;ForwardLineData&amp;gt;&amp;gt; &amp;amp;nextStageReserve;
  
  IcachePort icachePort;

  FetchQueue requests;
  FetchQueue transfers;
  IcacheState icacheState;
  InstSeqNum lineSeqNum;

  void evaluate();
  std::vector&amp;lt;Fetch1ThreadInfo&amp;gt; fetchInfo;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fetch1 is responsible for doing ITLB and ICache access. icachePort provides the interface between cache and fetch1. To learn more about ports refer &lt;a href=&#34;http://learning.gem5.org/book/part2/memoryobject.html#define-a-slave-port-type&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Overall the pipeline looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;pipeline.png&#34; alt=&#34;pipeline&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;the-execution-of-the-pipeline&#34;&gt;The execution of the pipeline&lt;/h1&gt;

&lt;p&gt;Now that we are aware of what are the key data-structures in each stage and how dis CPU connected to the pipeline and threads, we can look into how does the pipeline operate. Each pipeline has an event associated with it ???? which is scheduled for every clock tick. Whenever the event fires, the evaluate() function of the pipeline class is called. The evaluate method of the pipeline class calls the evaluate method on each of the pipeline stages in the reverse order. The order here is important because the updates from the later stages of the pipeline should be visible to the earlier stages of the pipeline. Think of a very simple stall logic, if the execute stage decides to stall in the current cycle, fetch1, fetch2, and decode all should stall and should not change any state in the current cycle. This would not be possible if we evaluate the fetch1, fetch2 and decode before evaluating execute stage. The converse of this is however not true, in conventional processor pipelines there are no feed-forward paths and hence the evaluation of later stages do not depend on the evaluation of earlier stages in the pipeline. Once all the stages are updated in the reverse order, the latches are advanced, which means in the two element buffer, the tail is pushed to the head and a space for new entry is made at the tail. The code looks somewhat like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;void                                                                     
Pipeline::evaluate()                                                     
{                                                                        
    /* Note that it&#39;s important to evaluate the stages in order to allow 
     *  &#39;immediate&#39;, 0-time-offset TimeBuffer activity to be visible from
     *  later stages to earlier ones in the same cycle */                
    execute.evaluate();                                                  
    decode.evaluate();                                                   
    fetch2.evaluate();                                                   
    fetch1.evaluate();                                                                                                     
                                                                         
    /* Update the time buffers/latches after the stages */               
    f1ToF2.evaluate(); 
    f2ToF1.evaluate();                                                   
    f2ToD.evaluate();                                                    
    dToE.evaluate();                                                     
    eToF1.evaluate();  
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the Latches &amp;ldquo;must&amp;rdquo; be updated after all the stages as otherwise the stages will start popping wrong data from the Latches. The order of updates within the Latches do not matter.&lt;/p&gt;

&lt;p&gt;Now that we have settled on what the order of updates for the pipeline stages should be lets delve deeper into what the evaluate() method for each of the pipeline stage is doing.&lt;/p&gt;

&lt;h2 id=&#34;execute-evaluation&#34;&gt;Execute evaluation&lt;/h2&gt;

&lt;p&gt;The evaluate method for the execute stage looks something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;void 
Execute::evaluate()
{
    // Set the dToE Latch data as the one to be pushed into input buffer next
    inputBuffer[inp.outputWire-&amp;gt;threadId].setTail(*inp.outputWire);

    lsq.step(); // Step the Load-Store Queues
    commit();   // Commit the instruction 
    issue();    // Issue instructions whose dependencies are satisfied 

    // Push the dToE Latch data into the input buffer
    inputBuffer[inp.outputWire-&amp;gt;threadId].pushTail();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The commit method looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;/** Commit takes an inst out from the head of the inflight inst queue and
  * depending upon whether out of order memory requests are supported or
  * not it commits the instruction, clears its destination entry from 
  * scorebiard and remove it from the head of the inflight insts 
*/
void
Execute::commit(){
    // While issue() has already issued some instructions in the FUs 
    while (executeInfo[tid].inFlightInsts-&amp;gt;empty() 
             &amp;amp;&amp;amp; num_insts_committed != commitLimit){

        head_inflight_inst = executeInfo[tid].inFlightInsts.front();
        inst = head_inflight_inst-&amp;gt;inst;
        mem_response = ( head_inflight_inst-&amp;gt;inst-&amp;gt;inLSQ ) ? 
                                     lsq.findResponse() : NULL;

        if (mem_response)
            handleMemResponse(inst,...);
        else {
	    // If there is a load/store inflight try to commit it before the
	    // the head of the inflight inst
            if (!executeInfo[tid].inFUMemInsts-&amp;gt;empty() &amp;amp;&amp;amp; lsq.canRequest()) {
                fun_inst = execeuteInfo.inFUMemInsts.front().inst;
                fu = funcUnits[ fun_inst-&amp;gt;fuIndex ];
                if (!fu_inst-&amp;gt;inLSQ &amp;amp;&amp;amp; fu_inst-&amp;gt;canEarlyIssue )
                {
                    try_to_commit = true;
                    inst = fu_inst;
                }
            }
	    // At this point depending on whether we are doing an early
	    // issue of mem request or actually handling the head of the 
	    // inflight inst. inst will be pointing to the corresponding inst
            if (!completed_inst &amp;amp;&amp;amp; !inst-&amp;gt;inLSQ) 
            {
                fu_inst = funcUnits[inst-&amp;gt;fuIndex]-&amp;gt;front();
                if (fu_inst.inst-&amp;gt;id == inst-&amp;gt;id){
                    try_to_commit = true;
                    completed_inst = true;
                }
            }
            if (try_to_commit)
            {
		// If there is a stream sequence mismatch i.e. inst is a post
		// branch inst, then discard the inst.
                discard_inst = 
                     inst-&amp;gt;id.streamSeqNum != executeInfo[tid].streamSeqNum;
                if (!discard_inst)
                    completed_inst = commitInst(...);
            
            }
            if (completed_inst)
            {
                funcUnits[inst-&amp;gt;fuIndex]-&amp;gt;stalled = false;
                executeInfo[tid].inFlightInsts-&amp;gt;pop();
                scoreboard[tid].clearInstDests(inst);
            }             
        }
    }
                 
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;void 
Execute::commitInst( inst, branch )
{
    if (inst-&amp;gt;isMemRef()) // load/store instruction
    {
        inst-&amp;gt;staticInst-&amp;gt;initiateAcc(); // this will eventually push inst on LSQ&#39;s request queue
	completed_inst = true;
    }
    else  // other arithmetic instructions
    {
	// Execute the instruction and write the result in register file
	// If the instruction is a branch instruction it updates the thread._pcState
	inst-&amp;gt;staticInst-&amp;gt;execute();

	target = thread-&amp;gt;pcState();
	pc_before = inst-&amp;gt;pc;
	if ( inst-&amp;gt;predictedTaken ){
	   if ( inst-&amp;gt;predictedTarget == target )
		// .. update branch variable for corect prediction
	   else
		// .. update for wrongly predicted 
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;/** Takes a set of instructions out to the inputBuffer. If the dependencies
 *  are satisfied keeps issuing the instructions bhy pushing them to FU 
 *  and marking the dests in scoreboard. It stops and removes the instruction
 *  from the inputBuffer in case all the insts in Latch are sent to the FUs. It
 *  also stops in the case when any of the inst is dependent on some previous
 *  inst. In this case any of insts after that are also not scheduled
*/
void 
Execute::issue ( inst )
{
    inst_in = inputBuffer[tid].front();

    do {
	issued = false;
	inst = insts_in-&amp;gt;insts[thread.inputIndex];
	for ( fu_idx = 0; fu_idx &amp;lt; numFuncUnits; fu_idx++ ){
	    fu = funcUnits[fu_idx];
	    if ( !fu-&amp;gt;stalled &amp;amp;&amp;amp; fu-&amp;gt;provides(inst) ){
		// Check scoreboard to see if inst depends on previous insts
		if ( scoreBoard.canInstIssue(inst) ){
		    fu-&amp;gt;push( inst );
		    // Mark the destination regs in the scoreboard
		    scoreBoard.markupInstDest(inst); 
		    executeInfo[tid].inFlightInsts.push(inst);
		    issued = true;
		}
	    }
	}
	if (executeInfo[tid].inputIndex == insts_in-&amp;gt;width() ){
	    inputBuffer[tid].pop();	
	    inst_in = NULL;
	}
    } while ( inst_in &amp;amp;&amp;amp; issued )
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fetch2-evaluation&#34;&gt;Fetch2 evaluation&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;void
fetch2::evaluate()
{
    // Mark the data in the f1ToF2 Lacth as the one to be 
    // pushed to the inputBuffer next
    inputBuffer[tid].setTail(*inp.OutputWire); // f1ToF2

    ForwardInstData &amp;amp;insts_out = *out.inputWire; // f2ToD
    BranchData &amp;amp;branch_inp = *branchInp.outputWire; // eToF1

    // React to branches from execute stage to update local branch
    // prediction structures (update the branch predictor itself)
    updateBranchPrediction(branch_inp);    

    // thread will be blocked if no space in decode&#39;s inputBuffer
    fetchInfo[tid].blocked = !nextStageReserve[tid].canReserve();

    if ( fetchInfo[tid].expectedStreamSeqNum == inputBuffer[tid].front()-&amp;gt;id.streamSeqNum 
	&amp;amp;&amp;amp; fetchInfp[tid].predictionSeqNum != inputBuffer[tid].front()-&amp;gt;id.predictionSeqNum )
		inputBuffer[tid].pop();

    line_in = inputBuffer[tid].front();
    
    // Discard the instructions for which Fetch2 predicted sequence number is 
    // different from the one with fetch1 fetched these instructions i.e. discard 
    // the instructions which are fetched not complying to branch pred decision in fetch2
    if ( line_in &amp;amp;&amp;amp; fetchInfo[tid].expectedStreamSeqNum == line_in-&amp;gt;id.streamSeqNum 
              &amp;amp;&amp;amp; fetchInfo[tid].predictionSeqNum != line_in-&amp;gt;id.predictionSeqNum) {
        inputBuffer.pop();
    }

    // fetch1 sends an entire caache line to fetch2 and not just a single inst
    // fetch2 depending on what the output width is decodes that many insts.
    // This decoding in hardware can be done using multiple decoders or
    // a single decoder time-multiplexed. 
    while ( line_in &amp;amp;&amp;amp; fetchInfo[tid].inputIndex &amp;lt; line_in-&amp;gt;lineWidth
             &amp;amp;&amp;amp; outputIndex &amp;lt; outputWidth )
    {
	fetchInfo.pc = line_in-&amp;gt;pc;
        dyn_inst = new MinorDynInst(line_in-&amp;gt;id);
	// Decode the instruction
        decoded_inst = decoder-&amp;gt;decode(fetchInfo[tid].pc);
	// Advance the PC
        TheISA::advancePC (fetchInfo[tid].pc, decoded_inst);
	// Calls branchPredictor.predict() if the inst is a Control inst
	// and updates prediction variable for new stream and prediction sequence numbers
        predictBranch(dyn_inst, prediction);
	insts_out.insts[output_index++] = dyn_inst;
	if ( !prediction.isBubble() )
	    line_in = NULL;
	else if ( fetchInfp[tid[.inputIndex == line_in-&amp;gt;lineWidth ) 
	    inputBuffer[tid].pop();
   }
   if ( !inst_out.isBubble() )
        nextStageReserve[tid].reserve();

   if ( !inp.outputWire-&amp;gt;isBubble )
        inputBuffer[inp.outputWire-&amp;gt;id.threadId].pushTail();        
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fetch1-evaluation&#34;&gt;Fetch1 evaluation&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;void
fetch1::evaluate()
{
    BranchData &amp;amp;execute_branch = *inp.outputWire;  // eToF1
    BranchData &amp;amp;fetch2_branch = *prediction.outputWire; // f2ToF1
    ForwardLineData &amp;amp;line_out = *out.inputWire; // f1ToF2

    fetchInfo[tid].blocked = !nextStageReserve[tid].canReserve();

    // Prioritize the branch stream change of execute over fetch2
    if ( execute_branch.isStreamChange() )
    {
	// updates the fetchInfo[tid].pc to branch target. updates the stream 
	// sequence number and prediction sequence number in fetcInfo[tid]
	changeStream( execute_branch )
    }
    else if ( fetch2_branch.isStreamChange() )
    {
	changeStream( fetch2_branch );
    }

    // Pushes the fetch memory request to the request queue, reserves a slot for
    // it in the transfers queue and does an ITLB access. 
    // It also updates the PC by +4 ( as no branch predictor in fetch1 ) 
    fetchLine(tid);

    nextStageReserve.reserve();

    // If the ICache is not busy, it tries to send fetch request to ICache
    // if successful it moves the request from request queue to transfers queue.
    stepQueues();

    // The head of the transfers queue is a completed fetch request
    if ( !transfers.empty() &amp;amp;&amp;amp; transfer.front()-&amp;gt;isComplete() )
    {
	line_out.pc = transfers.front().pc;
	line_out.line = transfers.front()-&amp;gt;data;
	popAndDiscard(transfers);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;instructions&#34;&gt;Instructions&lt;/h1&gt;

&lt;p&gt;Each cpu has its own class for dynamic instruction for e.g. for minor cpu this is defined in dyn_inst.cc as class MinorDynInst. Each Dynamic instruction class has an object of class StaticInst. StaticInst is an abstract class whose execute() method is purely virtual. It provides a base abstract class for all the different kinds of instructions in the ISA. Based on which ISA you are compiling gem5 with, the build system generates a class for each of the instructions in the ISA which is derived from StaticInst class and it has the execute method which can be used to execute the instruction and update the register file. Go to the file build/RISCV/arch/riscv/generated/decoder-ns.hh.inc and you will see all the classes for different instructions. The execute method for these classes are implemented in build/RISCV/arch/riscv/generated/exec-ns.cc.inc . To know more on how these classes got built with gem5 please refer to my tutorial on [How to add instruction to RISCV ISA and simulate on gem5](). gem5 minor cpu model does not read the registers while being issued to the functional unit pipelines. Instead, the function unit pipelines are just to model the delays and all the execution is done by calling execute() method in commitInst() method in the execute unit.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Computer Architectural Simulation Techniques</title>
      <link>https://nitish2112.github.io/post/event-driven-simulation/</link>
      <pubDate>Mon, 10 Jul 2017 10:09:32 -0400</pubDate>
      
      <guid>https://nitish2112.github.io/post/event-driven-simulation/</guid>
      <description>&lt;p&gt;This is an introduction tutorial on different types of simulation techniques and a brief overview of event driven simulators.
&lt;/p&gt;

&lt;h1 id=&#34;simulators&#34;&gt;&lt;strong&gt;Simulators&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;A simulator is a software performance model of a processor architecture which runs on a host machine. Simulation is cheap and much more accurate than analytical models.&lt;/p&gt;

&lt;h2 id=&#34;metrics-for-how-good-a-simulator-is&#34;&gt;&lt;strong&gt;Metrics for how good a simulator is&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;1) How accurate the simulation model is as compared to real hardware.&lt;/p&gt;

&lt;p&gt;2) How long does it take to develop the simulator,&lt;/p&gt;

&lt;p&gt;3) How much time does the simulator take to simulate the design, and&lt;/p&gt;

&lt;p&gt;4) How much coverage the simulator provides&lt;/p&gt;

&lt;h1 id=&#34;different-simulation-techniques&#34;&gt;&lt;strong&gt;Different Simulation Techniques&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&#34;functional-simulation&#34;&gt;&lt;strong&gt;Functional Simulation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Functional simulators only model the functional characteristics of a design. The instructions are simulated one at a time more of like an interpreter. These simulators are called ISA simulators. They are used to validate the correctness of a design rather than its performance characteristics. Some examples of ISA simulators would be spike for RISCV, Pydgin, SimpleScalar&amp;rsquo;s sim-safe etc.&lt;/p&gt;

&lt;h2 id=&#34;trace-driven-simulation&#34;&gt;&lt;strong&gt;Trace Driven Simulation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;A trace driven simulator takes program instructions and address traces and supplies them to micro-architectural simulator. Trace driven simulator separates functionality from timing. They require the user to store the trace files which can grow really large. However, trace driven simulators are not good in modelling multithreaded workloads as they cannot model the interaction between multiple threads.&lt;/p&gt;

&lt;h2 id=&#34;execution-driven-simulation&#34;&gt;&lt;strong&gt;Execution Driven Simulation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Execution driven simulators combine timing and functionality together. Depending upon the integration of functional and timing components execution driven simulators are classified as follows:&lt;/p&gt;

&lt;p&gt;1) &lt;strong&gt;Integrated&lt;/strong&gt; &amp;ndash; Timing and functional components are tightly coupled.&lt;/p&gt;

&lt;p&gt;2) &lt;strong&gt;Timing Directed&lt;/strong&gt; &amp;ndash; The functional model keeps track of the architectural state like register file, memory etc and timing model gets information from the functional model regarding various timing events like whether a load is a cache hit or miss and then it can account for the timing of that event.&lt;/p&gt;

&lt;p&gt;3) &lt;strong&gt;Functional First Simulation&lt;/strong&gt; &amp;ndash; Functional first simulators are more like Trace driven simulators. A functional simulator produces the traces which are at the same time fed to the timing simulator. The advantage of functional first simulator over trace driven simulators is that there is no need to store traces for functional first simulators.&lt;/p&gt;

&lt;p&gt;4) &lt;strong&gt;Timing First Simulation&lt;/strong&gt; &amp;ndash; In timing first simulators, the timing simulator models the architectural features ( register file, memory etc ). When the timing simulator commits an instruction the functional model checks it with the functional simulation. If there are any deviations, the functional simulator repairs the timing simulator by reloading the appropriate architectural state.&lt;/p&gt;

&lt;h1 id=&#34;event-driven-simulation&#34;&gt;&lt;strong&gt;Event driven simulation&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Event driven simulation falls more into the category of timing directed simulations. Event driven simulators consist of an event queue ( priority queue ) which is checked each clock cycle to see if any event is scheduled. Events and event queues model the timing part of the simulator. Whenever an event fires, it executes the process function that the object ( whose event fired ) has assigned to this particular event. Objects and their process methods constitute the functional part of the simulator.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Accelerating Face Detection on Programmable SoC Using C-Based Synthesis</title>
      <link>https://nitish2112.github.io/publication/face-detect/</link>
      <pubDate>Fri, 10 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://nitish2112.github.io/publication/face-detect/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Flexible and Dynamic Power Allocation in Broadband Multi-Beam Satellites</title>
      <link>https://nitish2112.github.io/publication/dynamic-power-satellite/</link>
      <pubDate>Fri, 10 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://nitish2112.github.io/publication/dynamic-power-satellite/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pointer-Chase Prefetcher</title>
      <link>https://nitish2112.github.io/project/prefetcher/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://nitish2112.github.io/project/prefetcher/</guid>
      <description>

&lt;h1 id=&#34;caches-only-exploit-spatial-and-temporal-locality-in-a-set-of-address-referenced-in-a-program-due-to-dynamic-construction-of-linked-data-structures-they-difficult-to-cache-as-the-spatial-locality-between-the-nodes-is-highly-dependent-on-the-data-layout-prefetching-can-play-an-important-role-in-improving-the-performance-of-linked-data-structures-in-this-project-a-pointer-chase-mechanism-along-with-compiler-hints-is-adopted-for-prefetching&#34;&gt;Caches only exploit spatial and temporal locality in a set of address referenced in a program. Due to dynamic construction of linked data-structures, they difficult to cache as the spatial locality between the nodes is highly dependent on the data layout. Prefetching can play an important role in improving the performance of linked data-structures. In this project a pointer chase mechanism along with compiler hints is adopted for prefetching.&lt;/h1&gt;

&lt;p&gt;image = &amp;ldquo;prefetcher.png&amp;rdquo;
caption = &amp;ldquo;My caption 😄&amp;rdquo;&lt;/p&gt;

&lt;p&gt;+++&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
